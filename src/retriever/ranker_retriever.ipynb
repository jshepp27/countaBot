{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### TODOs ###\n",
    "# DONE: Implement Semantic Ranking\n",
    "# TODOs: Commonsense Query and Concept Expansion: Topics, Concepts, Synonyms\n",
    "# TODOs: Targeted Retrieval with NLI over ADUs, Premises, Claims; discard non-ADUs\n",
    "\n",
    "# DONE: News Data\n",
    "# DONE: Add Concepts\n",
    "# DONE: Cosine Semantic Search\n",
    "# DONE: Prior Pre-processing, tokenization and sentence segmentation to speed processing\n",
    "# TODOs: Domain Restrict. Polarising social and political debate (Class labelling) only for higher-quality argument-knowledge set.\n",
    "# TODOs: News, Political, Sociology and 'Good', 'Positive' counter-evidence Knowledge Base.\n",
    "# TODOs: Bag of Topics Modelling\n",
    "# TODOs: Implement as a Class\n",
    "\n",
    "# TODOs: Keyphrase Selection\n",
    "# DONE: Manage Duplicate Keywords\n",
    "# DONE: Sentential Ranking\n",
    "# DONE: Include Topic Label\n",
    "# DONE: Include Concept Label\n",
    "# DONE: Add News\n",
    "# TODOs: Targeted Retreival with Semantic Graphs\n",
    "# TODOs: Target Argumentative Content Only\n",
    "# TODOs: Targeted Argument Content: Adus + Extractive Summary\n",
    "# TODOs: Query Expansion\n",
    "# TODOs: Multi-Field Search\n",
    "# TODOs: Additional News and Knowledge Sources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### INIT LOGGING ###\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ARGUMENT-EXTRACTOR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### INIT KNOWLEDGEBASE ###\n",
    "from src.utils.elastic_db import ElasticDB\n",
    "\n",
    "PORT = \"http://localhost:9200\"\n",
    "db = ElasticDB(elastic_port=PORT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### NLP FUNCTIONS ###\n",
    "from src.utils.utils import tokeniser, sentences_segment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### ADU CLASSIFIER ###\n",
    "# import os\n",
    "# path = \"/Users/joshua.sheppard/PycharmProjects/countaBot/\"\n",
    "# os.chdir(path)\n",
    "\n",
    "from src.detection.adu_classifier import predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATASETS ###\n",
    "import json\n",
    "import os\n",
    "\n",
    "root = \"/Users/joshua.sheppard/PycharmProjects/countaBot\"\n",
    "os.chdir(root)\n",
    "\n",
    "args = [json.loads(ln) for ln in open(\"./src/data/processed/cmv_processed.jsonl\")]\n",
    "mined_args = [json.loads(ln) for ln in open(\"./src/data/processed/cmv_argument_extraction.jsonl\")]\n",
    "topics = [json.loads(ln) for ln in open(\"./src/data/processed/argument_topic_concept.jsonl\")]\n",
    "concepts = [json.loads(ln) for ln in open(\"./src/data/processed/argument_concept.jsonl\")]\n",
    "\n",
    "# args = [json.loads(ln) for ln in open(\"../data/processed/cmv_processed.jsonl\")]\n",
    "# mined_args = [json.loads(ln) for ln in open(\"../data/processed/cmv_argument_extraction.jsonl\")]\n",
    "# topics = [json.loads(ln) for ln in open(\"../data/processed/argument_topic_concept.jsonl\")]\n",
    "# concepts = [json.loads(ln) for ln in open(\"../data/processed/argument_concept.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"ARGS:\", len(args), \" MINED-ARGS:\",  len(mined_args), \" TOPICS:\", len(topics), \" CONCEPTS: \", len(concepts))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### OPERATE ON A UNIQUE SET ###\n",
    "import pandas as pd\n",
    "\n",
    "# def unique_entries(args, key=\"id\"):\n",
    "#     data_ = pd.DataFrame(args)\n",
    "#     unique = data_.drop_duplicates(subset=key)\n",
    "#\n",
    "#     unique_ = []\n",
    "#     for _, i in unique.iterrows():\n",
    "#         unique_.append({\n",
    "#             \"id\": i[\"id\"],\n",
    "#             \"claim\": i[\"claim\"],\n",
    "#             \"argument\": i[\"argument\"],\n",
    "#             \"tgt_counter\": i[\"tgt_counter\"],\n",
    "#         })\n",
    "#\n",
    "#     return unique_\n",
    "#\n",
    "# unique_args = unique_entries(mined_args)\n",
    "# unique_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#len(unique_args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSPECT SUBJECT ARG ###\n",
    "import random\n",
    "_ = random.randint(0, len(mined_args))\n",
    "\n",
    "arg = \" \".join(i[\"sentence\"] for i in mined_args[_][\"argument\"])\n",
    "claim = mined_args[_][\"claim\"][\"sentence\"]\n",
    "\n",
    "#print(mined_args[_])\n",
    "print(_, \"\\n\")\n",
    "print(\"CLAIM: \", claim, \"\\n\")\n",
    "print(\"ARG: \", arg, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### KEYPHRASE EXTRACTORS ###\n",
    "from src.utils.keyphrase_extraction import yake_extract_keyphrase, summa_extract_keyphrase\n",
    "import keybert\n",
    "\n",
    "test = \"Brazil's minimum income has increasingly been accepted.\"\n",
    "ev_kp = yake_extract_keyphrase(test)\n",
    "ev_kp_ = summa_extract_keyphrase(test)\n",
    "\n",
    "test_2 = \" \"\n",
    "ev_kp_2 = yake_extract_keyphrase(test_2)\n",
    "ev_kp_2_ = summa_extract_keyphrase(test_2)\n",
    "\n",
    "print(ev_kp)\n",
    "print(ev_kp_)\n",
    "\n",
    "# Assert can Handel Blanks\n",
    "print(ev_kp_2)\n",
    "print(ev_kp_2_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_notion(notions_ids, notions_lst, arg_id, label):\n",
    "#     notion_id = notions_ids.index(arg_id)\n",
    "#     notion = notions_lst[notion_id][label]\n",
    "#     return str(notion) if notion else None\n",
    "#\n",
    "# topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"./src/data/processed/argument_topic_concept.jsonl\")]\n",
    "#\n",
    "# print(topic_ids.index(\"t3_30oi71\"))\n",
    "# print(topics[453])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "from src.detection.stance_classifier import sentence_stance, compare_stance\n",
    "from src.detection.stance_classifier import sentence_stance\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Disable Huggingface Logging\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"./src/data/processed/argument_topic_concept.jsonl\")]\n",
    "concept_ids = [json.loads(ln)[\"id\"] for ln in open(\"./src/data/processed/argument_concept.jsonl\")]\n",
    "\n",
    "def clean(phrase):\n",
    "    return re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", phrase)\n",
    "\n",
    "def get_notion(notions_ids, notions_lst, arg_id, label):\n",
    "    notion_id = notions_ids.index(arg_id)\n",
    "    notion = notions_lst[notion_id][label]\n",
    "    return str(notion) if notion else None\n",
    "\n",
    "### RETRIEVER ###\n",
    "db = db\n",
    "queries = []\n",
    "retrieved_ev = []\n",
    "\n",
    "# TODOs: Argumentative Sentence\n",
    "# TODOs: Query Expansion\n",
    "def search(mined, type=\"tgt_counter\", l=10):\n",
    "    id_ = mined[\"id\"]\n",
    "    claim = arg[\"claim\"]\n",
    "\n",
    "    topic = get_notion(topic_ids, topics, id_, \"topic_label\")\n",
    "    concept = get_notion(concept_ids, concepts, id_, \"concept_label\")\n",
    "\n",
    "    retrieved = []\n",
    "\n",
    "    adu_count = 0\n",
    "    targeted_response = []\n",
    "    for adu in mined[type]:\n",
    "\n",
    "        sentence = adu[\"sentence\"]\n",
    "        # if predict(sentence) != \"premise\":\n",
    "        #     # Count ADUs for reference\n",
    "        #     continue\n",
    "\n",
    "        # TODOs: Check this isn't overriding continue\n",
    "        adu_count += 1\n",
    "\n",
    "        #kp = extract_keyphrase(sentence)\n",
    "        kp = list(set(adu[\"kp\"]))\n",
    "\n",
    "        # TODOs: Common-sense Query Expansion\n",
    "        query = []\n",
    "        query.extend(kp)\n",
    "\n",
    "        # Ensure topics and concepts are unpacked (extended) into query list, as lists, else string will unpack 'l', 'i', 'k', 'e', 't'\n",
    "        query.extend([topic]) if topic else query\n",
    "        query.extend([concept]) if concept else query\n",
    "        query = list(set(query))\n",
    "\n",
    "        # Note: Now query becomes a string - be careful\n",
    "        query = \", \".join(i for i in query)\n",
    "        # print(query)\n",
    "\n",
    "        search = [(i[\"_source\"][\"document\"][\"source\"], i[\"_source\"][\"document\"][\"text\"]) for i in db.search(query_=query, k=l)]\n",
    "\n",
    "        source = [i[0] for i in search]\n",
    "        evidence = [i[1] for i in search]\n",
    "\n",
    "        #print(\"query\", query)\n",
    "        merged = \", \".join(i for i in evidence)\n",
    "        ev_kp = list(set(yake_extract_keyphrase(merged)))\n",
    "\n",
    "        retrieved.append({\"passages\": evidence, \"kp\": [clean(i) for i in ev_kp], \"source\": source})\n",
    "\n",
    "        targeted_response.append({\"sentence\": adu[\"sentence\"], \"selected_keyphrases\": []})\n",
    "\n",
    "    # TODOs: Implement yield without storing list\n",
    "    return ({\n",
    "        \"id\": id_,\n",
    "        \"claim\": claim,\n",
    "        \"argument\": mined[\"argument\"],\n",
    "        \"tgt_counter\": [i for i in targeted_response],\n",
    "        \"retrieved\": [i for i in retrieved],\n",
    "        \"adu_count\": adu_count\n",
    "    })\n",
    "\n",
    "# SINGLE ARGUMENT INSPECT\n",
    "# SAMPLE = unique_args[0]\n",
    "# results = search(SAMPLE)\n",
    "\n",
    "# tic = time.time()\n",
    "# SAMPLE = mined_args\n",
    "\n",
    "# retrieved_ev = []\n",
    "# with multiprocessing.Pool(8) as pool:\n",
    "#     with tqdm(total=(len(SAMPLE)), position=0, leave=True) as pbar:\n",
    "#         for arg in SAMPLE:\n",
    "#             retrieved_ev.append(search(arg))\n",
    "#             pbar.update()\n",
    "#     toc = time.time()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retrieved_ev[4]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "retrieved_ev_ = copy.deepcopy(retrieved_ev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duration = toc - tic\n",
    "print(\"TIME\", duration)\n",
    "retrieved_ev[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(retrieved_ev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODOs: Check Counter, Argument params pre-proccess\n",
    "# TODOs: Process Argument pairs fully; Constrain at train time\n",
    "\n",
    "_ = random.randint(0, len(mined_args))\n",
    "\n",
    "# NOTE: ADU Opinion Classifier reduces returned argument response. This is ok.\n",
    "print(\"Argument\", len(retrieved_ev[_][\"argument\"]), \"Retrieved\", len(retrieved_ev[70][\"retrieved\"]))\n",
    "\n",
    "# NOTE: ADU Opinion Classifier reduces returned argument response. This is ok.\n",
    "print(\"Counter\", len(retrieved_ev[_][\"tgt_counter\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# file_name = \"cmv_retrieved\"\n",
    "# fout = open(f\"./src/data/{file_name}.jsonl\", \"w\")\n",
    "#\n",
    "# #with fout:\n",
    "#     fout.write(json.dumps(retrieved_ev))\n",
    "#\n",
    "# logger.info(f\"[{len(retrieved_ev)} Data Stored as {file_name}.jsonl]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### REVIEW ###\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "retrieved_ev_ = [json.loads(ln) for ln in open(\"./src/data/cmv_retrieved.jsonl\", \"r\")][0]\n",
    "\n",
    "_ = random.randint(0, len(review))\n",
    "print(retrieved_ev_[_][\"argument\"], \"\\n\")\n",
    "print(retrieved_ev_[_][\"retrieved\"], \"\\n\")\n",
    "print(retrieved_ev_[_][\"tgt_counter\"], \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(retrieved_ev_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# TODOs: Discard equivalent stance, per sentence\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def cosine_similarity_(sentences):\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity()\n",
    "    scores = cos(embeddings[0], embeddings[1:])\n",
    "\n",
    "    scored = []\n",
    "    retrieved_sentences = sentences[1:]\n",
    "    for sent, similarity in zip(retrieved_sentences, scores):\n",
    "        scored.append((sent, similarity.numpy().item()))\n",
    "\n",
    "    return scored\n",
    "\n",
    "def rank_passages(ev, k=3):\n",
    "    \"\"\" return ranked passages using cosine-similarity between the input-argument and the retrieved passages\n",
    "        k determines the number of returned passages from the originally retrieved set.\n",
    "    \"\"\"\n",
    "    #adus = [i[\"sentence\"] for i in ev[\"argument\"]]\n",
    "    # Compare TGT with RETREIVED\n",
    "    adus = [i[\"sentence\"] for i in ev[\"tgt_counter\"]]\n",
    "    retrieved_passages = [i[\"passages\"] for i in ev[\"retrieved\"]]\n",
    "\n",
    "    #print(retrieved_passages)\n",
    "\n",
    "    # Merge\n",
    "    # Output 1 x merged sentences object per ADU sentence, with k collected passages as a list of sentences\n",
    "    merged_passages = []\n",
    "    for passages in retrieved_passages:\n",
    "        merged_sents = []\n",
    "        # Iterate n x sentences for each k=5 retrieved passages\n",
    "        for passage in passages:\n",
    "            # Segment as a list of sentences\n",
    "            sents = sentences_segment(passage)\n",
    "            # Add sentences to merged_sentences object\n",
    "            merged_sents.extend(sents)\n",
    "\n",
    "        # Store merged sentence object for each ADU\n",
    "        merged_passages.append(merged_sents)\n",
    "\n",
    "    rank_retrieved = []\n",
    "    # Rank n x merged sentences for each 1 x ADU\n",
    "    for adu, merged in zip(adus, merged_passages):\n",
    "        scored = []\n",
    "        sentences = [adu]\n",
    "        sentences.extend(merged)\n",
    "        scored = cosine_similarity_(sentences)\n",
    "\n",
    "        ranked_sents = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select top-k sentences\n",
    "        ranked_sents = ranked_sents[0:k]\n",
    "\n",
    "        merged = \", \".join(i[0] for i in ranked_sents)\n",
    "        merged_kp = yake_extract_keyphrase(merged)\n",
    "        rank_retrieved.append({\"ranked_passages\": merged, \"kp\": merged_kp})\n",
    "\n",
    "    #print(\"\\n RANKED\", rank_retrieved)\n",
    "    return rank_retrieved\n",
    "\n",
    "# TODOs: Join passages and sentence rank\n",
    "### SCORE COSINE SIMILARITY ###\n",
    "tic = time.time()\n",
    "retrieved_ranked = copy.deepcopy(retrieved_ev_)\n",
    "counta = 0\n",
    "with tqdm(total=(len(retrieved_ev_)), position=0, leave=True) as pbar:\n",
    "    for i in range(0, len(retrieved_ev_)):\n",
    "        counta += 1\n",
    "        retrieved_ranked[i][\"retrieved\"] = [i for i in rank_passages(retrieved_ev_[i])]\n",
    "        pbar.update()\n",
    "\n",
    "toc = time.time()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retrieved_ranked_ = copy.deepcopy(retrieved_ranked)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subject = retrieved_ranked[2]\n",
    "\n",
    "for i in subject:\n",
    "    for _, j in zip(subject[\"tgt_counter\"], subject[\"retrieved\"]):\n",
    "        print(\"COUNTER: \", _, \"\\n\")\n",
    "        print(\"EVIDENCE: \",j, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duration = toc - tic\n",
    "print(duration)\n",
    "len(retrieved_ranked)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NOTE: Zipping retrieved evidence, args\n",
    "print(len(retrieved_ranked), len(retrieved_ev), len(args))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = random.randint(0, len(retrieved_ev))\n",
    "retrieved_ranked[_]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = \"cmv_rr_\"\n",
    "fout = open(f\"./src/data/processed/{file_name}.jsonl\", \"w\")\n",
    "\n",
    "# Deep_copies\n",
    "rr = copy.deepcopy(retrieved_ranked)\n",
    "\n",
    "with tqdm(total=(len(rr))) as pbar:\n",
    "    with fout:\n",
    "        for ln in rr:\n",
    "            fout.write(json.dumps(ln))\n",
    "            pbar.update()\n",
    "\n",
    "logger.info(f\"[{len(rr)} Data Stored as {file_name}.jsonl]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### INSPECT OUTPUT ###\n",
    "rr_ = [json.loads(ln) for ln in open(\"./src/data/processed/cmv_rr.jsonl\", \"r\")][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subject = rr_[random.randint(0, len(rr_))]\n",
    "\n",
    "print(\"CLAIM: \", subject[\"claim\"][\"sentence\"])\n",
    "print(\"===========================================\\n\")\n",
    "for i, j, k in zip(subject[\"argument\"], subject[\"tgt_counter\"], subject[\"retrieved\"]):\n",
    "    print(\"ARG: \", i[\"sentence\"], \"\\n\")\n",
    "    print(\"COUNTER: \", j[\"sentence\"], \"\\n\")\n",
    "    print(\"EVIDENCE: \", clean(k[\"ranked_passages\"]).lower(), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DONE: Keyphrase Selection\n",
    "# TODOs: Full-run, arguments\n",
    "import copy\n",
    "\n",
    "### KEYPHRASE SELECTION OBJECT ###\n",
    "_rr = copy.deepcopy(rr_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(phrase):\n",
    "    return re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", phrase)\n",
    "\n",
    "def cosine_similarity_(sentences):\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    cos = torch.nn.CosineSimilarity()\n",
    "    scores = cos(embeddings[0], embeddings[1:])\n",
    "\n",
    "    scored = []\n",
    "    retrieved_sentences = sentences[1:]\n",
    "    for sent, similarity in zip(retrieved_sentences, scores):\n",
    "        scored.append((sent, similarity.numpy().item()))\n",
    "\n",
    "    return scored\n",
    "\n",
    "def selected_keyphrases(arg):\n",
    "    kps = [_[\"kp\"] for _ in arg[\"retrieved\"]]\n",
    "    tgt_sentences = [_[\"sentence\"] for _ in arg[\"tgt_counter\"]]\n",
    "\n",
    "    selected_kps = []\n",
    "    for tgt, kp in zip(tgt_sentences, kps):\n",
    "        vectors = [tgt]\n",
    "        vectors.extend(kp)\n",
    "\n",
    "        similarity = cosine_similarity_(vectors)\n",
    "        #print(\"Before: \", [i[0] for i in similarity])\n",
    "        selected = [i[0] for i in similarity if i[1] > 0.2]\n",
    "\n",
    "        selected_kps.append(list(set(selected)))\n",
    "\n",
    "    for _, j in zip(arg[\"tgt_counter\"], selected_kps):\n",
    "        _[\"selected_keyphrases\"] = j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### SELECTED KEYPHRASES ###\n",
    "import tqdm as tqdm\n",
    "\n",
    "SAMPLE = _rr\n",
    "with tqdm.tqdm_notebook(total=(SAMPLE), position=0, leave=True) as pbar:\n",
    "    for arg in SAMPLE:\n",
    "        selected_keyphrases(arg)\n",
    "        pbar.update()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = random.randint(0, 9)\n",
    "_rr[_][\"tgt_counter\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WORK WITH DEEP COPIES\n",
    "\n",
    "def overlap_kp(string, sub):\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = string.find(sub, start) + 1\n",
    "        if start > 0:\n",
    "            count+=1\n",
    "        else:\n",
    "            return count\n",
    "\n",
    "## WORKING, YET REPLACED WITH SIMILARITY ###\n",
    "\n",
    "# DONE: Similarity rank\n",
    "# DONE: Add Stopwords\n",
    "# stop = [i.strip() for i in open(\"./src/data/lexicon/stopwords.txt\")]\n",
    "# def selected_keyphrases(arg):\n",
    "#     kps = [_[\"kp\"] for _ in arg[\"retrieved\"]]\n",
    "#     tgt_sentences = [_[\"sentence\"] for _ in arg[\"tgt_counter\"]]\n",
    "#\n",
    "#     selected_kps = []\n",
    "#\n",
    "#     # Iterate per target sentence\n",
    "#     for tgt, kp in zip(tgt_sentences, kps):\n",
    "#         selected = []\n",
    "#\n",
    "#         for terms in kp:\n",
    "#             singletons = terms.split()\n",
    "#             for single in singletons:\n",
    "#                 if single in stop:\n",
    "#                     continue\n",
    "#                 if overlap_kp(tgt.lower(), single) > 0:\n",
    "#                     selected.append(terms)\n",
    "#\n",
    "#         selected_kps.append(list(set(selected)))\n",
    "#\n",
    "#     for _, j in zip(arg[\"tgt_counter\"], selected_kps):\n",
    "#         _[\"selected_keyphrases\"] = j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### TOY EXAMPLE: COMPATING SIMILARITY ###\n",
    "\n",
    "counter = [\"i dont see a comment on puberty yet so ill weigh in a bit.there is absolutely no reason to believe that all transwomen athletes have an advantage over their female counterparts as your assertion makes.the defining biological trigger that gives males an advantage over females is the presence of high amounts of testosterone\"]\n",
    "\n",
    "phrases = [\n",
    "    'assertion makes the defining biological',\n",
    "    'makes the defining biological trigger',\n",
    "    'high amounts of testosterone',\n",
    "    'assertion makes the defining',\n",
    "    'makes the defining biological',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding = counter\n",
    "embedding.extend(phrases)\n",
    "\n",
    "embedding\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(embedding)\n",
    "result = cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[1:]\n",
    ")\n",
    "\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### EXTRACT LABELLED INSTANCES ###\n",
    "# def extract_labelled(dict_):\n",
    "#     res = []\n",
    "#     count = 0\n",
    "#     for i in dict_.items():\n",
    "#         j, k = i\n",
    "#\n",
    "#         if k[\"selectec_keyphrases\"] != []:\n",
    "#             #res.append({\"id\": i[0], \"argument\": k[\"argument\"], \"label\": k[\"label\"]})\n",
    "#             res.append(count)\n",
    "#     return res\n",
    "#\n",
    "# counts = []\n",
    "# for i in rr_train_:\n",
    "#     sents = [_[\"sentence\"] for _ in i[\"tgt_counter\"]]\n",
    "#     empties = [_ for _ in i[\"tgt_counter\"] if _[\"selected_keyphrases\"] != []]\n",
    "#     counts.append((sents, empties))\n",
    "\n",
    "# for i in rr_train_[0][\"tgt_counter\"]:\n",
    "#     print(i[\"selected_keyphrases\"])\n",
    "\n",
    "for i in rr_train_[\"tgt_counter\"]:\n",
    "    print(i)\n",
    "\n",
    "    break\n",
    "    # print(i[\"selected_keyphrases\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in rr_train_:\n",
    "    print(i[\"tgt_counter\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "}import random\n",
    "_ = random.randint(0, len(rr_train))\n",
    "rr_train_[_][\"tgt_counter\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = \"cmv_rr_selected\"\n",
    "fout = open(f\"./src/data/processed/{file_name}.jsonl\", \"w\")\n",
    "\n",
    "# Deep_copies\n",
    "import copy\n",
    "rr_selected = copy.deepcopy(rr_train_)\n",
    "\n",
    "with fout:\n",
    "    fout.write(json.dumps(rr_selected))\n",
    "    fout.write(\"\\n\")\n",
    "\n",
    "logger.info(f\"[{len(rr_selected)} Data Stored as {file_name}.jsonl]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### PREVIOUS RETRIEVER ###\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# import multiprocessing\n",
    "# from src.detection.stance_classifier import sentence_stance, compare_stance\n",
    "# from src.utils_.word_net_expansion import expand_query\n",
    "# from src.detection.stance_classifier import sentence_stance\n",
    "# # from multiprocessing.pool import ThreadPool as Pool\n",
    "# import time\n",
    "#\n",
    "# # Disable Huggingface Logging\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#\n",
    "# ### RETRIEVER ###\n",
    "# db = db\n",
    "# queries = []\n",
    "# retrieved_ev = []\n",
    "#\n",
    "# topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_topic_concept.jsonl\")]\n",
    "# concept_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_concept.jsonl\")]\n",
    "#\n",
    "# # TODOs: Argumentative Sentence\n",
    "# # TODOs: Query Expansion\n",
    "# def search(mined):\n",
    "#     id_ = mined[\"id\"]\n",
    "#     # print(\"\\n\", id_)\n",
    "#\n",
    "#     topic = arg[\"argument\"][0][\"topic\"]\n",
    "#     concept = arg[\"argument\"][0][\"concept\"]\n",
    "#\n",
    "#     retrieve_len = 5\n",
    "#     retrieved = []\n",
    "#\n",
    "#     # for adu in mined[\"argument\"]:\n",
    "#     for adu in mined[\"tgt_counter\"]:\n",
    "#\n",
    "#         sentence = adu[\"sentence\"]\n",
    "#         if len(tokeniser(sentence)) <= 8:\n",
    "#             continue\n",
    "#\n",
    "#         kp = list(set(adu[\"kp\"]))\n",
    "#         #print(kp)\n",
    "#         # topic = adu[\"topic\"]\n",
    "#         # concept = adu[\"concept\"]\n",
    "#\n",
    "#         kp.append(topic) if topic else kp\n",
    "#         kp.append(concept) if concept else kp\n",
    "#         # print(kp)\n",
    "#\n",
    "#         query = \", \".join(i for i in kp)\n",
    "#\n",
    "#         search = [(i[\"_source\"][\"document\"][\"source\"], i[\"_source\"][\"document\"][\"text\"]) for i in db.search(query_=query, k=retrieve_len)]\n",
    "#\n",
    "#         source = [i[0] for i in search]\n",
    "#         evidence = [i[1] for i in search]\n",
    "#\n",
    "#         merged = \", \".join(i for i in evidence)\n",
    "#         ev_kp = list(set(yake_extract_keyphrase(merged)))\n",
    "#\n",
    "#         retrieved.append({\"passages\": evidence, \"kp\": [i for i in ev_kp], \"source\": source})\n",
    "#\n",
    "#     # TODOs: Implement yield without storing list\n",
    "#     return ({\n",
    "#         \"id\": id_,\n",
    "#         \"argument\": mined[\"argument\"],\n",
    "#         \"tgt_counter\": mined[\"tgt_counter\"],\n",
    "#         \"retrieved\": [i for i in retrieved],\n",
    "#     })\n",
    "#\n",
    "# # SINGLE ARGUMENT INSPECT\n",
    "# # SAMPLE = unique_args[0]\n",
    "# # results = search(SAMPLE)\n",
    "#\n",
    "# tic = time.time()\n",
    "# SAMPLE = unique_args[0:100]\n",
    "#\n",
    "# retrieved_ev = []\n",
    "# with tqdm(total=(len(SAMPLE)), position=0, leave=True) as pbar:\n",
    "#     for arg in SAMPLE:\n",
    "#         retrieved_ev.append(search(arg))\n",
    "#         pbar.update()\n",
    "# toc = time.time()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### QUERY EXPANSION ###\n",
    "# TODOs: ConceptNet, pre-processing\n",
    "# TODOs: WordNet, faster, also pre-processing\n",
    "# from src.utils_.concept_net_expansion import ConceptNet\n",
    "# sample = mined_args[5]\n",
    "#\n",
    "# concept_net = ConceptNet(api=\"http://api.conceptnet.io/\", l=5)\n",
    "# print(sample[\"claim\"][\"kp\"])\n",
    "# concept_ = sample[\"claim\"][\"kp\"][0]\n",
    "\n",
    "# terms = concept_.split()\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "# stop = [i.strip() for i in open(\"./src/data/lexicon/stopwords.txt\")]\n",
    "# #print(stop)\n",
    "#\n",
    "# terms_ = [i for i in terms if i not in stop]\n",
    "# expansion = [concept_net.get_similar(i) for i in terms_]\n",
    "#\n",
    "# expansion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fout = open(\"../data/cmv_rr.jsonl\", \"w\")\n",
    "#\n",
    "# args = [json.loads(ln) for ln in open(\"../data/cmv_processed.jsonl\")]\n",
    "# sample = args[0:sample]\n",
    "#\n",
    "# # for i, j in zip(retrieved_ranked, sample):\n",
    "# #     # Add counter to the dictionary (implicitly, i)\n",
    "# #     i[\"counter\"] = j[\"counter\"]\n",
    "# #     fout.write(json.dumps(i))\n",
    "# #     fout.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OLD\n",
    "# kp = adu[\"kp\"]\n",
    "# topic = adu[\"topic\"]\n",
    "# concept = adu[\"concept\"]\n",
    "#\n",
    "# kp.append(topic) if topic else kp\n",
    "# kp.append(concept) if concept else kp\n",
    "#\n",
    "# query = \", \".join(i for i in adu[\"kp\"])\n",
    "# print(query)\n",
    "#\n",
    "# # TODOs: Add title field for all ES indices to enable multi-field search\n",
    "# search = [(i[\"_source\"][\"document\"][\"source\"], i[\"_source\"][\"document\"][\"text\"]) for i in db.search(query_=query, k=retrieve_len)]\n",
    "#\n",
    "# #evidence = [i[1] for i in search]\n",
    "# #source = [i[0] for i in search]\n",
    "#\n",
    "# evidence = [i[1] for i in search]\n",
    "# ev_kp = yake_extract_keyphrase(evidence)\n",
    "#\n",
    "#         # try:\n",
    "#         #     ev_kp = yake_extract_keyphrase(evidence)\n",
    "#         # except:\n",
    "#         #     ev_kp = [\" \"]\n",
    "#\n",
    "#         #retrieved.append({\"passages\": evidence, \"kp\": [i for i in ev_kp], \"source\": source})\n",
    "#         #retrieved.append({\"passages\": evidence})\n",
    "#\n",
    "#\n",
    "#\n",
    "#     # TODOs: Implement yield without storing list\n",
    "#     return ({\n",
    "#         \"id\": id_,\n",
    "#         \"argument\": mined[\"argument\"],\n",
    "#         \"retrieved\": [i for i in retrieved]\n",
    "#     })\n",
    "#\n",
    "# for arg in unique_args[0:10]:\n",
    "#     retrieved_ev.append(search(arg))\n",
    "#\n",
    "# # SAMPLE = unique_args[0:100]\n",
    "# #\n",
    "# # step = max(int(len(SAMPLE) / 10), 1)\n",
    "# # BATCHES = [SAMPLE[i:i + step] for i in range(0, len(SAMPLE), step)]\n",
    "# #\n",
    "# # retrieved_ev = []\n",
    "# # for idx, batch in enumerate(BATCHES):\n",
    "# #     print('-' * 25 + 'Batch %d/%d' % (idx + 1, len(batch)) + '-' * 25)\n",
    "# #\n",
    "# #     with multiprocessing.Pool(8) as pool:\n",
    "# #         with tqdm(total=(len(batch))) as pbar:\n",
    "# #             for arg in batch:\n",
    "# #                 retrieved_ev.append(search(arg))\n",
    "# #                 pbar.update()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SINGLE SEARCH FUNCTION\n",
    "####\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# import multiprocessing\n",
    "# from src.detection.stance_classifier import sentence_stance, compare_stance\n",
    "# from src.utils_.word_net_expansion import expand_query\n",
    "# from src.detection.stance_classifier import sentence_stance\n",
    "# # from multiprocessing.pool import ThreadPool as Pool\n",
    "# import time\n",
    "#\n",
    "# # Disable Huggingface Logging\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#\n",
    "# ### RETRIEVER ###\n",
    "# db = db\n",
    "# queries = []\n",
    "# retrieved_ev = []\n",
    "#\n",
    "# topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_topic_concept.jsonl\")]\n",
    "# concept_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_concept.jsonl\")]\n",
    "#\n",
    "# import random\n",
    "# _ = random.randint(0, len(unique_args))\n",
    "# sample = unique_args[_]\n",
    "#\n",
    "# # TODOs: Argumentative Sentence\n",
    "# def search(mined):\n",
    "#     id_ = mined[\"id\"]\n",
    "#     print(\"\\n\", id_)\n",
    "#\n",
    "#     retrieve_len = 5\n",
    "#     retrieved = []\n",
    "#     for adu in mined[\"argument\"]:\n",
    "#\n",
    "#         sentence = adu[\"sentence\"]\n",
    "#         if len(tokeniser(sentence)) <= 8:\n",
    "#             continue\n",
    "#\n",
    "#         kp = list(set(adu[\"kp\"][0:5]))\n",
    "#         topic = adu[\"topic\"]\n",
    "#         concept = adu[\"concept\"]\n",
    "#\n",
    "#         kp.append(topic) if topic else kp\n",
    "#         kp.append(concept) if concept else kp\n",
    "#\n",
    "#         query = \", \".join(i for i in adu[\"kp\"])\n",
    "#         print(query)\n",
    "#         print(\" \")\n",
    "#         search = [(i[\"_source\"][\"document\"][\"source\"], i[\"_source\"][\"document\"][\"text\"]) for i in db.search(query_=query, k=retrieve_len)]\n",
    "#\n",
    "#         source = [i[0] for i in search]\n",
    "#         evidence = [i[1] for i in search]\n",
    "#\n",
    "#         merged = \", \".join(i for i in evidence)\n",
    "#         ev_kp = yake_extract_keyphrase(merged)\n",
    "#         print(ev_kp)\n",
    "#\n",
    "#         retrieved.append({\"passages\": evidence, \"kp\": [i for i in ev_kp], \"source\": source})\n",
    "#\n",
    "#     # TODOs: Implement yield without storing list\n",
    "#     return ({\n",
    "#         \"id\": id_,\n",
    "#         \"argument\": mined[\"argument\"],\n",
    "#         \"retrieved\": [i for i in retrieved]\n",
    "#     })\n",
    "#\n",
    "# result = search(sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ### CHECK BLANKS ###\n",
    "# args_ = [json.loads(ln)[\"argument\"][\"argument\"] for ln in open(\"../data/cmv_processed.jsonl\")]\n",
    "# ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/cmv_processed.jsonl\")]\n",
    "#\n",
    "# for j, k in zip(args_, ids):\n",
    "#     if j == \"\":\n",
    "#         print(\"blanks\", j, k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from src.detection.stance_classifier import sentence_stance, compare_stance\n",
    "# from src.utils_.word_net_expansion import expand_query\n",
    "# from src.detection.stance_classifier import sentence_stance\n",
    "# import multiprocessing\n",
    "# import json\n",
    "# import time\n",
    "#\n",
    "# # TODOs: Adu, Counter + KP Extraction as 'Argument Mining' preprocessing module\n",
    "#\n",
    "# # Disable Huggingface Logging\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#\n",
    "# topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_topic_concept.jsonl\")]\n",
    "# concept_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_concept.jsonl\")]\n",
    "#\n",
    "# def get_notion(notions_ids, notions_lst, arg_id, label):\n",
    "#     notion_id = notions_ids.index(arg_id)\n",
    "#     notion = notions_lst[notion_id][label]\n",
    "#     return str(notion) if notion else None\n",
    "#\n",
    "# def extract_adus(arg_):\n",
    "#     arg, id_ = arg_\n",
    "#     print(\"\\n\", id_)\n",
    "#\n",
    "#     topic = get_notion(topic_ids, topics, id_, \"topic_label\")\n",
    "#     concept = get_notion(concept_ids, concepts, id_, \"concept_label\")\n",
    "#\n",
    "#     adu_sents = sentences_segment(arg)\n",
    "#\n",
    "#     adus = []\n",
    "#     for _ in adu_sents:\n",
    "#         if len(tokeniser(_)) <= 8:\n",
    "#             continue\n",
    "#\n",
    "#         try:\n",
    "#             kp = extract_keyphrase(_)\n",
    "#         except:\n",
    "#             kp = [\" \"]\n",
    "#\n",
    "#         kp.append(topic) if topic else kp\n",
    "#         kp.append(concept) if concept else kp\n",
    "#         print(kp)\n",
    "#\n",
    "#         adu = {\"sentence\": _, \"kp\": [i for i in kp], \"stance\": sentence_stance(_, kp[0])}\n",
    "#\n",
    "#         adus.append(adu)\n",
    "#\n",
    "#     yield ({\n",
    "#         \"id\": id_,\n",
    "#         \"argument\": [i for i in adus]\n",
    "#     })\n",
    "#\n",
    "# step = max(int(len(unique) / 10), 1)\n",
    "# batches = [unique[i:i + step] for i in range(0, len(unique), step)]\n",
    "#\n",
    "# mined_args = []\n",
    "# # TODOs: Remove Huggingface Warnings\n",
    "#\n",
    "# for idx, batch in enumerate(batches):\n",
    "#     print('-' * 25 + 'Batch %d/%d' % (idx + 1, len(batches)) + '-' * 25)\n",
    "#\n",
    "#     with multiprocessing.Pool(8) as pool:\n",
    "#         with tqdm(total=(len(batch))) as pbar:\n",
    "#             for arg in batch:\n",
    "#                 mined_args.append([i for i in extract_adus(arg)])\n",
    "#                 pbar.update()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_topic(arg_id):\n",
    "#     topic_id = topic_ids.index(arg_id)\n",
    "#     topic = topics[topic_id][\"topic_label\"]\n",
    "#     return str(topic) if topic else None\n",
    "#\n",
    "# def get_concept(arg_id):\n",
    "#     concept_id = concept_ids.index(arg_id)\n",
    "#     concept = concepts[concept_id][\"concept_label\"]\n",
    "#     return str(concept) if concept else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "# TODOs: Fix Vectorizer Issue\n",
    "# kb = KeyBERT()\n",
    "# vectorizer = KeyphraseCountVectorizer()\n",
    "# def extract_keyphrase(doc, n_gram=3, n_kp=3, use_mmr=\"False\", use_maxsum=\"False\"):\n",
    "#     try:\n",
    "#         kp = kb.extract_keywords(doc, keyphrase_ngram_range=(0, 3), stop_words=\"english\", diversity=0.3,)\n",
    "#         kp_ = kb.extract_keywords(doc, vectorizer=vectorizer, stop_words=\"english\", diversity=0.3)\n",
    "#\n",
    "#     except:\n",
    "#         return [\" \"]\n",
    "#\n",
    "#     # Concatonate, remove duplicates\n",
    "#     kp = kp + kp_\n",
    "#     kp = [i[0] for i in kp]\n",
    "#     kp = list(set(kp))\n",
    "#\n",
    "#     return kp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # TODOs: Compute in Batches\n",
    "# sample = unique[0:100]\n",
    "# with multiprocessing.Pool(8) as pool:\n",
    "#     with tqdm(total=(len(unique))) as pbar:\n",
    "#         for arg in unique:\n",
    "#             mined_args.append([i for i in extract_adus(arg)])\n",
    "#             pbar.update()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def retrieved_evidence(mined, retrieve_len=5):\n",
    "#     \"\"\" Retrieves Evidence from Knowledge base, returning a well-formed Retrieved Evidence Object\n",
    "#     given an input Argument\"\"\"\n",
    "#\n",
    "#     id_ = mined[\"id\"]\n",
    "#     print(\"\\n\", id_)\n",
    "#\n",
    "#     retrieved = []\n",
    "#     adus = []\n",
    "#     for _ in mined[\"argument\"]:\n",
    "#         if len(tokeniser(_)) <= 8:\n",
    "#             continue\n",
    "#\n",
    "#         kp = extract_keyphrase(_)\n",
    "#         print(kp)\n",
    "#         adu = {\"sentence\": _, \"kp\": [i for i in kp], \"stance\": sentence_stance(_, kp[0])}\n",
    "#\n",
    "#         kp.append(topic) if topic else kp\n",
    "#         kp.append(concept) if concept else kp\n",
    "#\n",
    "#         query = \", \".join(i for i in kp)\n",
    "#         print(query)\n",
    "#\n",
    "#         # TODOs: Add title field for all ES indices to enable multi-field search\n",
    "#         search = [(i[\"_source\"][\"document\"][\"source\"], i[\"_source\"][\"document\"][\"text\"]) for i in db.search(query_=query, k=retrieve_len)]\n",
    "#\n",
    "#         evidence = [i[1] for i in search]\n",
    "#         source = [i[0] for i in search]\n",
    "#\n",
    "#         ev_kp = extract_keyphrase(evidence)\n",
    "#\n",
    "#         retrieved.append({\"passages\": evidence, \"kp\": [i[0] for i in ev_kp], \"source\": source})\n",
    "#         #retrieved.append({\"passages\": evidence, \"source\": source})\n",
    "#         adus.append(adu)\n",
    "#\n",
    "#     return ({\n",
    "#         \"id\": id_,\n",
    "#         \"argument\": [i for i in adus],\n",
    "#         \"retrieved\": [i for i in retrieved]\n",
    "#     })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# import time\n",
    "#\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def cosine_similarity_(sentences):\n",
    "#     embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "#\n",
    "#     cos = torch.nn.CosineSimilarity()\n",
    "#     scores = cos(embeddings[0], embeddings[1:])\n",
    "#\n",
    "#     scored = []\n",
    "#     retrieved_sentences = sentences[1:]\n",
    "#     for sent, similarity in zip(retrieved_sentences, scores):\n",
    "#         scored.append((sent, similarity.numpy().item()))\n",
    "#\n",
    "#     return scored\n",
    "#\n",
    "# def rank_passages(ev, k=3):\n",
    "#     adus = [i[\"sentence\"] for i in ev[\"argument\"]]\n",
    "#     retrieved_passages = [i[\"passages\"] for i in ev[\"retrieved\"]]\n",
    "#\n",
    "#     # Merge\n",
    "#     # Output 1 x merged sentences object per ADU sentence, with k=5 collected passages as a list of sentences\n",
    "#     merged_passages = []\n",
    "#     for passages in retrieved_passages:\n",
    "#         merged_sents = []\n",
    "#         # Iterate n x sentences for each k=5 retrieved passages\n",
    "#         for passage in passages:\n",
    "#             # Segment as a list of sentences\n",
    "#             sents = sentences_segment(passage)\n",
    "#             # Add sentences to merged_sentences object\n",
    "#             merged_sents.extend(sents)\n",
    "#\n",
    "#         # Store merged sentence object for each ADU\n",
    "#         merged_passages.append(merged_sents)\n",
    "#\n",
    "#     rank_retrieved = []\n",
    "#     # Rank n x merged sentences for each 1 x ADU\n",
    "#     for adu, merged in zip(adus, merged_passages):\n",
    "#         scored = []\n",
    "#         sentences = [adu]\n",
    "#         sentences.extend(merged)\n",
    "#         scored = cosine_similarity_(sentences)\n",
    "#\n",
    "#         ranked_sents = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "#\n",
    "#         # Select top-k sentences\n",
    "#         ranked_sents = ranked_sents[0:k]\n",
    "#\n",
    "#         merged = \", \".join(i[0] for i in ranked_sents)\n",
    "#         merged_kp = extract_keyphrase(merged)\n",
    "#         rank_retrieved.append({\"ranked_passages\": merged, \"kp\": merged_kp})\n",
    "#\n",
    "#     return rank_retrieved\n",
    "#\n",
    "# # TODO: Clean text\n",
    "# # TODO: Collect unique Keyphrases per Argument\n",
    "# rank_passages(retrieved_ev[0])\n",
    "# #print(len(test[\"ranked_passages\"][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handle duplicates\n",
    "# def rank_passages(ev, k=3):\n",
    "#     \"\"\" Handles a Retrieved Evidence Object, returning the top-k passages for each ADU \"\"\"\n",
    "#     # Per Argument\n",
    "#     # Index into Retrieved Evidence Object\n",
    "#     adus = [i for i in ev[0][\"argument\"]]\n",
    "#     retrieved = [i for i in ev[0][\"retrieved\"]]\n",
    "#\n",
    "#     #print(len(retrieved), len(adus))\n",
    "#\n",
    "#     # Rank k-returned passages for each ADU\n",
    "#     r_retrieved = []\n",
    "#     for adu, passage in zip(adus, retrieved):\n",
    "#         scored = []\n",
    "#         ranked_ev = []\n",
    "#         for _, kp in zip(passage[\"evidence\"], passage[\"kp\"]):\n",
    "#             scored.append((_, kp, cosine_similarity(str(adu), str(_))))\n",
    "#\n",
    "#         scored = sorted(scored, key=lambda x: x[2], reverse=True)[0:3]\n",
    "#         for i, j, k in scored:\n",
    "#             ranked_ev.append({\"evidence\": i, \"kp\": j, \"similarity\": k})\n",
    "#\n",
    "#         r_retrieved.append(ranked_ev)\n",
    "#\n",
    "#     return r_retrieved\n",
    "\n",
    "# 1 Argument x 4 ADUs x 5 Retrieved Passages\n",
    "# ranked = [i for i in rank_passages(retrieved_ev[3])]\n",
    "# print(len(ranked))\n",
    "# print(ranked)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch\n",
    "# import time\n",
    "#\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#\n",
    "# # TODOs: Join passages and sentence rank\n",
    "# ### SCORE COSINE SIMILARITY ###\n",
    "# def cosine_similarity(sent_1, sent_2):\n",
    "#     sentences = [sent_1, sent_2]\n",
    "#     embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "#\n",
    "#     cos = torch.nn.CosineSimilarity(dim=0)\n",
    "#     score = cos(embeddings[0], embeddings[1])\n",
    "#\n",
    "#     return score.numpy().item()\n",
    "#\n",
    "# ### SCORE TF-KEYWORD OVERLAP ###\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \", \".join([i for i in adu_kp]).find(j) != -1: score += 1\n",
    "#\n",
    "#             else: continue\n",
    "#     return score\n",
    "#\n",
    "# ### RANK PASSAGES ###\n",
    "# def score_passages(ev):\n",
    "#     for _ in range(0, len(ev[\"argument\"])):\n",
    "#         print(_)\n",
    "#\n",
    "# from collections import defaultdict\n",
    "# def rank_passages(ev, k=2):\n",
    "#     adus = [i for i in ev[\"argument\"]]\n",
    "#     retrieved = [i for i in ev[\"retrieved\"]]\n",
    "#\n",
    "#     rank_retrieved = []\n",
    "#     count = 0\n",
    "#\n",
    "#     for adu, passages in zip(adus, retrieved):\n",
    "#         count += 1\n",
    "#         scored = []\n",
    "#\n",
    "#         # 5 passages\n",
    "#         for passage in passages[\"passages\"]:\n",
    "#             score = cosine_similarity(str(adu), str(passage))\n",
    "#             scored.append((passage, score))\n",
    "#\n",
    "#         ranked_passages = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "#         ranked_passages = ranked_passages[0:k]\n",
    "#\n",
    "#         merged = \", \".join(i[0] for i in ranked_passages)\n",
    "#         merged_kp = extract_keyphrase(merged)\n",
    "#         rank_retrieved.append({\"ranked_passages\": merged, \"kp\": merged_kp})\n",
    "#\n",
    "#     return rank_retrieved\n",
    "#\n",
    "# import copy\n",
    "# ### UPDATE RETRIEVED OBJECT ###\n",
    "# # for i in range(0, len(retrieved_ev)):\n",
    "# #     retrieved_ranked[i][\"retrieved\"] = [i for i in rank_passages(retrieved_ev[i])]\n",
    "#\n",
    "# tic = time.time()\n",
    "# retrieved_ranked = copy.deepcopy(retrieved_ev)\n",
    "# with tqdm(total=(len(retrieved_ev)), position=0, leave=True) as pbar:\n",
    "#     for i in range(0, len(retrieved_ev)):\n",
    "#         retrieved_ranked[i][\"retrieved\"] = [i for i in rank_passages(retrieved_ev[i])]\n",
    "#     pbar.update()\n",
    "#\n",
    "# toc = time.time()\n",
    "# # duration = toc - tic\n",
    "#\n",
    "# retrieved_ranked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fix KW extraction\n",
    "# Fix Duplicates\n",
    "# def rank_passages(ev, k=2):\n",
    "#     adus = [i for i in ev[\"argument\"]]\n",
    "#     retrieved = [i for i in ev[\"retrieved\"]]\n",
    "#\n",
    "#     rank_retrieved = []\n",
    "#     count = 0\n",
    "#\n",
    "#     for adu, passages in zip(adus, retrieved):\n",
    "#         count += 1\n",
    "#         scored = []\n",
    "#\n",
    "#         # 5 passages\n",
    "#         for passage in passages[\"passages\"]:\n",
    "#             score = cosine_similarity(str(adu), str(passage))\n",
    "#             scored.append((passage, score))\n",
    "#\n",
    "#         ranked_passages = sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "#         ranked_passages = ranked_passages[0:k]\n",
    "#\n",
    "#         merged = \", \".join(i[0] for i in ranked_passages)\n",
    "#         merged_kp = extract_keyphrase(merged)\n",
    "#         rank_retrieved.append({\"ranked_passages\": merged, \"kp\": merged_kp})\n",
    "#\n",
    "#     return rank_retrieved\n",
    "#\n",
    "# ev = retrieved_ev[0]\n",
    "# rank_passages(ev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def fuck_you():\n",
    "#     print(\"fuck you\")\n",
    "#\n",
    "# fuck_you()\n",
    "#\n",
    "# def rank_(ev):\n",
    "#     # Index into Retrieved Evidence Object\n",
    "#     ev = ev[0]\n",
    "#     adus = [i for i in ev[0][\"argument\"]]\n",
    "#     retrieved = [i for i in ev[0][\"retrieved\"]]\n",
    "#     k = 3\n",
    "#     print(\"hello\")\n",
    "#     # # Rank k-returned passages for each ADU\n",
    "#     # count = 0\n",
    "#     # r_retrieved = []\n",
    "#     # for adu, passage in zip(adus, retrieved):\n",
    "#     #     count += 1\n",
    "#     #     ranked_passages = []\n",
    "#     #     for _ in passage[\"evidence\"]:\n",
    "#     #         print(_)\n",
    "#     #         ranked_passages.append((_, cosine_similarity(adu, _)))\n",
    "#     #         r_retrieved.append({\"evidence\": i, \"similarity\": k} for i, k in sorted(ranked_passages, key=lambda x: x[1], reverse=True)[0:k])\n",
    "#     #         print(r_retrieved)\n",
    "#\n",
    "#     # return {\n",
    "#     #     \"r_retrieved\": r_retrieved\n",
    "#     # }\n",
    "#\n",
    "# #print(rank_(retrieved_ev[0:1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from multiprocessing.pool import ThreadPool as Pool\n",
    "# from yake import KeywordExtractor\n",
    "# import tqdm.notebook as tqdm\n",
    "# import time\n",
    "# from summa import keywords\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# ### PASSAGE RANKING; KEYWORD OVERLAP ###\n",
    "# kw_extractor = KeywordExtractor(lan=\"en\", n=3, top=5)\n",
    "#\n",
    "# # TODOs: For each ADU, Rank Merged Evidence using Keyword Overlap and Filter for Contrasting Stance\n",
    "# # TODOs: Handel Multiple Keywords\n",
    "#\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "#     # TODOs: Robust 'None' handeling\n",
    "#     if adu_kp == None:\n",
    "#         return score\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     else:\n",
    "#         for i in evidence_kp:\n",
    "#             for j in i.split():\n",
    "#                 # Ensure string value, to enact .find\n",
    "#                 if \", \".join([i for i in adu_kp]).find(j) != -1: score += 1\n",
    "#\n",
    "#                 else: continue\n",
    "#\n",
    "#     return score\n",
    "#\n",
    "# def calculate_overlap(merged_ev, adu_kp):\n",
    "#\n",
    "#     for ev_unit in sentences_segment(merged_ev):\n",
    "#         toks = tokeniser(ev_unit)\n",
    "#         kp_overlap = 0\n",
    "#\n",
    "#         if len(toks) <= 8: continue\n",
    "#\n",
    "#         #ev_unit_kp = [i for i in keywords.keywords(ev_unit).split(\"\\n\")]\n",
    "#         ev_unit_kp = [i[0] for i in kw_extractor.extract_keywords(ev_unit)]\n",
    "#\n",
    "#         if ev_unit_kp:\n",
    "#             kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "#\n",
    "#         else: ev_unit_kp = None\n",
    "#         yield ev_unit, ev_unit_kp, kp_overlap\n",
    "#\n",
    "# # pool = Pool(8)\n",
    "# ### RANK PASSAGES ###\n",
    "# def score_passages(ev_):\n",
    "#     adu = ev_[0][\"argument_discourse_unit\"]\n",
    "#     adu_stance = ev_[0][\"adu_stance\"]\n",
    "#     merged_ev = ev_[0][\"merged_evidence\"]\n",
    "#     adu_kp = ev_[0][\"adu_keyphrases\"]\n",
    "#\n",
    "#     ### CALCULATE OVERLAP ###\n",
    "#     for ev_unit, ev_unit_kp, kp_overlap in calculate_overlap(merged_ev, adu_kp):\n",
    "#         target = adu_kp[0]\n",
    "#\n",
    "#         compared_stace = compare_stance(ev_unit, target)\n",
    "#         if compared_stace != adu_stance:\n",
    "#             yield {\n",
    "#                 \"adu\": adu,\n",
    "#                 \"adu_kp\": adu_kp,\n",
    "#                 \"evidence_unit\": ev_unit,\n",
    "#                 \"evidence_kps\": ev_unit_kp,\n",
    "#                 \"overlap\": kp_overlap,\n",
    "#                 \"evidence_stance\": compare_stance(ev_unit, target),\n",
    "#                 \"adu_stance\": adu_stance\n",
    "#             }\n",
    "#\n",
    "#         else: continue\n",
    "#\n",
    "# ### SCORED EVIDENCE ###\n",
    "# def score_evidence(retrieved_evidence):\n",
    "#     for ev_ in retrieved_ev:\n",
    "#         yield [i for i in score_passages(ev_)]\n",
    "#\n",
    "# ### RANKED EVIDENCE ###\n",
    "# def rank_filter_counter_evidence(retireved_evidence, k=3):\n",
    "#     with tqdm(total=(len(retrieved_ev))) as pbar:\n",
    "#         for i in score_evidence(retrieved_ev):\n",
    "#             yield sorted(i, key=lambda y: y[\"overlap\"], reverse=True)[0:k]\n",
    "#\n",
    "#             pbar.update()\n",
    "#\n",
    "#\n",
    "# ### SELECT TOP-K COUNTER-EVIDENCE ###\n",
    "# tic = time.time()\n",
    "# ranked_sorted_evidence = [i for i in rank_filter_counter_evidence(retrieved_ev)]\n",
    "# ranked_sorted_evidence\n",
    "# toc = time.time()\n",
    "#\n",
    "# print(toc - tic)\n",
    "# # TIME 1:20M"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# idx = 2\n",
    "# for ln in retrieved_ev:\n",
    "#     r = ln[0]\n",
    "#     for _ in range(0, len(r[\"argument\"])):\n",
    "#         print(r[\"argument\"][_][\"sentence\"])\n",
    "#         print(r[\"argument\"][_][\"kp\"])\n",
    "#         print(\"\")\n",
    "#         print(r[\"retrieved\"][_][\"evidence\"])\n",
    "#         print(r[\"retrieved\"][_][\"kp\"])\n",
    "\n",
    "#\"counter\": {\"counter\": arg[\"counter\"][\"counter\"], \"counter_kp\": arg[\"counter\"][\"counter_keyphrases\"]}\n",
    "# \"argument_discourse_unit\": adu,\n",
    "# \"query\": query,\n",
    "# \"adu_keyphrases\": [i for i in kp],\n",
    "# \"adu_stance\": sentence_stance(adu, kp),\n",
    "# \"merged_evidence\": \", \".join(ln for ln in evidence)\n",
    "# \"retrieved_documents_titles\": titles,\n",
    "# \"retrieved_evidence\": evidence,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # TODOs: Speed-up, Parrelleise, Yield\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: score += 1\n",
    "\n",
    "#             else: continue\n",
    "\n",
    "#     return score\n",
    "\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity']\n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "\n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "\n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu,\n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp,\n",
    "#         \"kp_overlap\": kp_overlap\n",
    "\n",
    "#         })\n",
    "\n",
    "# adu_ev_overlap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### OVERLAP RANKED EVIDENCE ###\n",
    "\n",
    "# adu_ev_overlap.sort(key=lambda y: y[\"kp_overlap\"], reverse=True)\n",
    "# adu_ev_overlap\n",
    "\n",
    "# ### FILTER IRRELEVANT EVIDENCE ###\n",
    "# overlapping = [i for i in adu_ev_overlap if i[\"kp_overlap\"] !=0]\n",
    "\n",
    "# len(adu_ev_overlap), len(overlapping)\n",
    "# overlapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stance Test\n",
    "# adu = 'I cant remember the topic that spurred this discussion but a friend and I were debating whether manmade things were natural.'\n",
    "# ev_unit = 'In this essay, Mill argues the idea that the morality of an action can be judged by whether it is natural or unnatural.'\n",
    "# target = 'natural things'\n",
    "#\n",
    "# stance = compare_stance(ev_unit, target)\n",
    "# stance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ASSERT SAME STANCE ###\n",
    "# from detection.stance_classifier import sentence_stance, compare_stance\n",
    "#\n",
    "# # TODOs: Ensure KPs Extracts are constrained to 1 unit\n",
    "# opposing_stance = []\n",
    "# for i in overlapping:\n",
    "#     adu = i[\"adu\"]\n",
    "#     target = \" \".join(i for i in i[\"adu_kp\"])\n",
    "#     ev_unit = i[\"ev_unit\"]\n",
    "#\n",
    "#     ev_stance = compare_stance(ev_unit, ev_unit, target)\n",
    "#     adu_stance = sentence_stance(adu, target)\n",
    "#\n",
    "#     if ev_stance != adu_stance:\n",
    "#         opposing_stance.append((ev_unit, ev_stance, adu_stance))\n",
    "#\n",
    "#     else: continue\n",
    "#\n",
    "# opposing_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RANKING ###\n",
    "\n",
    "# TODOs: Speed-up, Parrelleise, Yield\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity'] \n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "    \n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "    \n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu, \n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp, \n",
    "#         \"kp_overlap\": kp_overlap\n",
    "        \n",
    "#         })\n",
    "        \n",
    "# adu_ev_overlap\n",
    "\n",
    "\n",
    "#rank_passages(retrieved_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # TODOs: Package as a Module\n",
    "# # TODOs: Handle Negation (Polarity shifters)\n",
    "# # TODOs: Review Unsuperived Approach; Consider adveanced patterns and common-sence knowledge\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# sentence = \"I hate abortion rights. Abortions should be banned.\"\n",
    "# sentence_2 = \"I like abortion rights. I belive we should keep them.\"\n",
    "# sentence_3 = \"I hate tennis. People should play tennis more often\"\n",
    "\n",
    "# ### STANCE SCORING ###\n",
    "\n",
    "# # TODOs: https://www.cs.uic.edu/~liub/FBS/opinion-mining-final-WSDM.pdf \n",
    "# # TODOs: Pattern based Negation\n",
    "# # TODOs: Semantic Orientation of an opinion (Claim)\n",
    "# # TODOs:Group synonyms of 'features', 'targets'\n",
    "\n",
    "# phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# ### SENTIMENT LEXICONS ###\n",
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/negative_lex.txt\")]\n",
    "# polarity_shifters = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/shifter_lexicon.txt\")]\n",
    "\n",
    "# ### STANCE: ASPECT-SEMANTIC ORIENTATION ###\n",
    "# def extract_aspect(sentence, n_gram):\n",
    "#     aspects = extract_keyphrase(str(sentence))[0]\n",
    "\n",
    "#     return nlp(aspects)\n",
    "\n",
    "# def index_aspect(phrase, aspect, sentence):    \n",
    "#     patterns = [nlp(aspect)]\n",
    "#     phrase_matcher.add(phrase, None, *patterns)\n",
    "\n",
    "#     start = 0\n",
    "#     stop = 0\n",
    "\n",
    "#     matched_phrases = phrase_matcher(sentence)\n",
    "#     for i in matched_phrases:\n",
    "#         _, start, stop = i\n",
    "        \n",
    "#     return start, stop\n",
    "\n",
    "# # TODOs: Implement Polarity Shifters, Simple\n",
    "# # TODOs: Implement Polarity Shifters, Complex, Verb Patterns\n",
    "# def stance_score(start, stop, sentence):\n",
    "#     pos_score = 0.0\n",
    "#     neg_score = 0.0\n",
    "\n",
    "#     score = 0\n",
    "#     for idx, tok in enumerate(sentence):\n",
    "#         if idx == start or idx == stop:\n",
    "#             continue\n",
    "\n",
    "#         # TODOs: Implement Polarity Shift\n",
    "#         # TODOs: Experiement with descriptive term + keyphrase aspects\n",
    "#         # TODOs: ABSA https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis\n",
    "#         # Use external libaray: Textblob\n",
    "        \n",
    "#         k = 8\n",
    "#         # Negation Rules\n",
    "#         shifted_tok = None\n",
    "#         shifted_toks = []\n",
    "\n",
    "#         if (tok.dep_ == \"neg\") or (tok.dep_ in polarity_shifters):\n",
    "#             #Shift to Negative\n",
    "#             if idx <= k:\n",
    "#                 if idx < start: neg_score += 1/(start - idx)\n",
    "#                 else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#             if shifted_tok != None and shifted_tok in neg:\n",
    "#                 print(shifted_tok.text)\n",
    "#                 # Shift to Positive\n",
    "#                 if idx < start: pos_score += 1/(start - idx)\n",
    "#                 elif idx > start: pos_score += 1/(idx - stop)**0.5\n",
    "#                 else: continue\n",
    "\n",
    "#         # Aspect Sentement Orientation\n",
    "#         if tok.text in pos:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "            \n",
    "#             if idx < start: pos_score += 1/(start - idx)\n",
    "#             else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#         if tok.text in neg:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "\n",
    "#             if idx <= start: neg_score += 1/(start - idx)\n",
    "#             else: neg_score += 1/(idx - stop)**0.5\n",
    "    \n",
    "#     score = pos_score - neg_score /(pos_score + neg_score + 1)\n",
    "\n",
    "#     return score\n",
    "\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "    \n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 score += 1\n",
    "#                 token = j\n",
    "            \n",
    "#             else: continue\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def get_overlapping_token(evidence_kp, adu_kp):\n",
    "#     for i in evidence_kp:\n",
    "#         overlap_tokens = []\n",
    "#         for j in i.split():\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 overlap_tokens.append(j) \n",
    "            \n",
    "#         return \" \".join(i for i in overlap_tokens)\n",
    "\n",
    "# def sentence_stance(sentence, aspect):\n",
    "#     sentence = nlp(sentence)\n",
    "\n",
    "#     start, stop = index_aspect(\"aspects\", aspect, sentence)\n",
    "#     score = stance_score(start, stop, sentence)\n",
    "\n",
    "#     # Add Neutral\n",
    "#     #stance = {\"claim\": sentence, \"stance\": \"PRO\", \"aspect\": aspect} if score > 0 else {\"claim\": sentence, \"stance\": \"CON\", \"aspect\": aspect}\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# def fuzzy_match(target, evidence_unit):\n",
    "\n",
    "#     overlapping_aspect = process.extractOne(target, ev.split())[0]\n",
    "#     score = overlapping_aspect[1]\n",
    "\n",
    "#     overlapping_aspect = nlp(re.sub(r'[^\\w]', ' ', overlapping_aspect))\n",
    "\n",
    "#     return overlapping_aspect, score\n",
    "\n",
    "# def compare_stance(ev_unit, evidence_aspect, adu_target):\n",
    "#     # Note: Already identified mathcing or partially matching Aspects. \n",
    "\n",
    "#     # Get the overlapping evidence aspect-target.\n",
    "#     overlapping_target, score = fuzzy_match(target=adu_aspect, evidence_unit=ev)\n",
    "    \n",
    "#     # Get position of the overlapping_target\n",
    "#     start, stop = index_aspect(\"OVERLAP\", nlp(overlapping_target), nlp(ev_unit))\n",
    "\n",
    "#     # Assert Stance towards evidence aspect\n",
    "#     score = stance_score(start, stop, nlp(ev_unit))\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# ev = \"These simple ideas and techniques could help both you and your lover enjoy sex. 1 / 10 Getty Images/Caiaimage Think beyond the thrust.\"\n",
    "# ev_aspect = \"sex\", \"relationship\", \"opportunity\"\n",
    "\n",
    "# adu = 'Hello! Let me preface by saying I dont believe there is a better sex.'\n",
    "# adu_aspect = \"better sex\"\n",
    "\n",
    "# print(sentence_stance(\"The mutual trust and understanding you share with your partner will lead to better sex, but that's not the only reason sex can be better when you're not in a relationship.\", adu_aspect))\n",
    "# print(compare_stance(ev, ev_aspect, adu_aspect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.matcher import DependencyMatcher, Matcher\n",
    "# matcher = Matcher(vocab=nlp.vocab)\n",
    "# matcher\n",
    "\n",
    "# # Matching Rule: Pronouns with Verbs that follow them\n",
    "# aspect = \"better sex\"\n",
    "# patterns = [\n",
    "#     [{\"DEP\": \"neg\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"DEP\": \"neg\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"LOWER\": aspect.lower()}]\n",
    "# ]\n",
    "\n",
    "# test = nlp(\"Hello! Let me preface by saying I dont believe there is a not better sex.\")\n",
    "# test_2 = nlp(\"These simple ideas and techniques could help both you and your lover enjoy better sex.\")\n",
    "\n",
    "# matcher.add(\"test\", patterns=patterns)\n",
    "# result = matcher(test_2, as_spans=True)\n",
    "\n",
    "# result\n",
    "\n",
    "# # for tok in test:\n",
    "# #     print(tok.i, tok, tok.pos_, tok.dep_, tok.head.i, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TARGETED RETRIEVAL: ATTACKING PEMISES ###\n",
    "\n",
    "# from BERT_adu_classifier import predict\n",
    "\n",
    "# premises = []\n",
    "# for sent in sentences:\n",
    "#     prediction = predict(sent)\n",
    "    \n",
    "#     if prediction == \"premise\":\n",
    "#         premises.append(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b747fec5972a5a28202124dfae2950631b4721a6e18efe99aaae23c73408484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
