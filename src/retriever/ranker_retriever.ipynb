{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils_.elastic_db:Connecting to http://localhost:9200 \n",
      "INFO:src.utils_.elastic_db:Connected to <Elasticsearch(['http://localhost:9200'])> \n"
     ]
    }
   ],
   "source": [
    "from src.utils_.elastic_db import ElasticDB\n",
    "\n",
    "# INIT DB OBJECT\n",
    "PORT = \"http://localhost:9200\"\n",
    "db = ElasticDB(elastic_port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD DATASETS ###\n",
    "import json\n",
    "import random\n",
    "\n",
    "args = [json.loads(ln) for ln in open(\"../data/cmv_processed.jsonl\")]\n",
    "topics = [json.loads(ln) for ln in open(\"../data/argument_topic_concept.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(5990, 10303)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics), len(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "There Are Only Two Genders \n",
      "\n",
      "So to go ahead and clear something before you guys start thinking Im transphobic Im not. I support females having sex changes and males having sex changes if they choose to do so.There are only two genders and sexes. Can there be feminine males and masculine females? Yes absolutelyMales can like the color pink and like other feminine products but they are still a man because of their genitalia. Females can like trucks and like hunting and other masculine things but they are still a female.Now can a female become a male?\n"
     ]
    }
   ],
   "source": [
    "### SUBJECT ARG ###\n",
    "import random\n",
    "_ = random.randint(0, 99)\n",
    "\n",
    "claim = args[_][\"claim\"]\n",
    "arg = args[_][\"argument\"][\"argument\"]\n",
    "\n",
    "print(_)\n",
    "print(claim, \"\\n\")\n",
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "### TODOs ###\n",
    "\n",
    "# TODOs: News Data\n",
    "# TODOs: Add Concepts\n",
    "# TODOs: Commonsense Query and Concept Expansion: Topics, Concepts, Synonyms\n",
    "# TODOs: Cosine Semantic Search\n",
    "# TODOs: Research: Evidence Retrieval, Infor Retrieval, Context Aware, Neural Retrieval\n",
    "# TODOs: Targeted Retrieval with NLI over ADUs, Premises, Claims; discard non-ADUs.\n",
    "# TODOs: Parallel process\n",
    "# TODOs: Prior tokenization and sentence segmentation to speed processing\n",
    "# TODOs: Domain Restrict. Polarising social and political debate (Class labelling) only for higher-quality argument-knowledge set.\n",
    "# TODOs: News, Political, Sociology and 'Good', 'Positive' counter-evidence Knowledge Base.\n",
    "# TODOs: Bag of Topics Modelling\n",
    "\n",
    "# TODOs: Implement as a Class\n",
    "# TODOs: Implement Logging\n",
    "\n",
    "# TODOs: Implement Semantic Search: https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch\n",
    "# TODOs: Implement Semantic Ranking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'my', 'name', 'is', 'Josh', '!']\n",
      "['hello, my name is Josh!', 'How are you doing today?', \"I'm curious ... will this line seperate?\", \"I'm not so sure Dr.\", 'Evil']\n"
     ]
    }
   ],
   "source": [
    "### NLP FUNCTIONS ###\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokeniser(doc):\n",
    "    return word_tokenize(doc)\n",
    "\n",
    "def sentences_segment(doc):\n",
    "    return sent_tokenize(doc)\n",
    "\n",
    "# Test Statements\n",
    "print(tokeniser(\"hello, my name is Josh!\"))\n",
    "print(sentences_segment(\"hello, my name is Josh! How are you doing today? I'm curious ... will this line seperate? I'm not so sure Dr. Evil\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "kb = KeyBERT()\n",
    "def extract_keyphrase(doc, n_gram=3, n_kp=3, use_mmr=\"False\", use_maxsum=\"False\"):\n",
    "    #kp = kb.extract_keywords(doc, vectorizer=KeyphraseCountVectorizer(), stop_words=\"english\", diversity=0.2,)\n",
    "\n",
    "    kp = kb.extract_keywords(doc, keyphrase_ngram_range=(0, 4), stop_words=\"english\", diversity=0.2,)\n",
    "\n",
    "    return [i[0] for i in kp[0:n_kp]] if kp else None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.037s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.031s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.027s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.027s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.035s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.037s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.032s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.048s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.017s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.038s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.029s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.040s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.026s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.044s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.026s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.033s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.039s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.037s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.110s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.102s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.158s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.038s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.016s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.049s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.028s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.007s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.022s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.041s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.012s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.169s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.013s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.041s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.030s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.023s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.156s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.015s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.054s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/*/_search [status:200 duration:0.033s]\n"
     ]
    }
   ],
   "source": [
    "from src.detection.stance_classifier import sentence_stance, compare_stance\n",
    "from src.utils_.word_net_expansion import expand_query\n",
    "from src.detection.stance_classifier import sentence_stance\n",
    "\n",
    "### RETRIEVER ###\n",
    "db = db\n",
    "queries = []\n",
    "retrieved_ev = []\n",
    "\n",
    "topic_ids = [json.loads(ln)[\"id\"] for ln in open(\"../data/argument_topic_concept.jsonl\")]\n",
    "def get_topic(arg_id):\n",
    "    topic_id = topic_ids.index(arg_id)\n",
    "    topic = topics[topic_id][\"topic_label\"]\n",
    "    return str(topic) if topic else None\n",
    "\n",
    "# TODOs: Add News\n",
    "# TODOs: Include Topic Label\n",
    "# TODOs: Include Concept Label\n",
    "# TODOs: Query Expansion\n",
    "# TODOs: Multi-Field Search\n",
    "def retrieved_evidence(arg, query_expansion=True, retrieve_len=5):\n",
    "    id_ = arg[\"id\"]\n",
    "    topic = get_topic(id_)\n",
    "\n",
    "    counters_sents = sentences_segment(arg[\"counter\"][\"counter\"])\n",
    "    adu_sents = sentences_segment(arg[\"argument\"][\"argument\"])\n",
    "\n",
    "    # Retrieve per ADU\n",
    "    results = []\n",
    "    retrieved = []\n",
    "    adus = []\n",
    "    for _ in adu_sents:\n",
    "        if len(tokeniser(_)) <= 8:\n",
    "            continue\n",
    "\n",
    "        kp = extract_keyphrase(_)\n",
    "        adu = {\"sentence\": _, \"kp\": [i for i in kp], \"stance\": sentence_stance(_, kp[0])}\n",
    "\n",
    "        kp.append(topic) if topic else kp\n",
    "        query = \", \".join(i for i in kp)\n",
    "        search = [(i[\"_source\"][\"document\"][\"title\"], i[\"_source\"][\"document\"][\"text\"])for i in db.search(query_=query, k=retrieve_len)]\n",
    "\n",
    "        evidence = [i[1] for i in search]\n",
    "        retrieved.append({\"evidence\": evidence, \"kp\": [i for i in extract_keyphrase(evidence)]})\n",
    "        adus.append(adu)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": arg[\"id\"],\n",
    "        \"argument\": [i for i in adus],\n",
    "        \"retrieved\": [i for i in retrieved]})\n",
    "\n",
    "    return results\n",
    "\n",
    "retrieved_ev = []\n",
    "for arg in args[0:10]:\n",
    "    retrieved_ev.append(retrieved_evidence(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 't3_30oi71', 'argument': [{'sentence': 'Section I Why is Basic Income Increasingly Popular?', 'kp': ['basic income increasingly popular', 'basic income increasingly', 'section basic income increasingly'], 'stance': 'NEUTRAL'}, {'sentence': 'Basic income is a policy that has broad support from both the progressive left and libertarian right.', 'kp': ['basic income', 'basic income policy', 'basic income policy broad'], 'stance': 'PRO'}, {'sentence': 'Centerleft economists including Paul Krugman have endorsed the scheme for various reasons.', 'kp': ['paul krugman endorsed scheme', 'economists including paul krugman', 'krugman endorsed scheme various'], 'stance': 'PRO'}, {'sentence': 'BI also reduces inequality by redistributing income from capital to labor.', 'kp': ['bi reduces inequality redistributing', 'reduces inequality redistributing income', 'inequality redistributing income capital'], 'stance': 'CON'}], 'retrieved': [{'evidence': ['Brazil. Minimum income has been increasingly accepted by the Brazilian government. In 2004, President\\xa0Lula da Silva signed into law a bill to establish a universal basic income.', \"Committee member Lady Rhys-Williams argued that the incomes for adults should be more like a basic income. She was also the first to develop the negative income tax model. Her son Brandon Rhys Williams proposed a basic income to a parliamentary committee in 1982, and soon after that in 1984, the Basic Income Research Group, now the Citizen's Basic Income Trust, began to conduct and disseminate research on basic income.\", 'Many technology experts and technology entrepreneurs have begun endorsing basic income in the 2000s and 2010s. These include Marshal Brain, Sam Altman, James Hughes, Facebook co-founder Chris Hughes, Elon Musk, and Mark Zuckerberg (in his 2017 Harvard commencement speech), and Jeremy Rifkin. The overriding theme among technologists who favor basic income is the belief that automation is creating an increasingly unstable labor market.', \"Several British academics have been involved in the basic income debate. Among them the following:Organisations. The organisation Basic Income UK is 'a collective of independent people promoting unconditional basic income as a progressive social policy for the United Kingdom, and beyond'.\", 'Basic Income. Delaney opposes implementing a basic income (also known as a universal basic income). Minimum wage.'], 'kp': [('brazil minimum income increasingly', 0.7746), ('williams proposed basic income', 0.7498), ('technologists favor basic income', 0.6414)]}, {'evidence': [\"Several British academics have been involved in the basic income debate. Among them the following:Organisations. The organisation Basic Income UK is 'a collective of independent people promoting unconditional basic income as a progressive social policy for the United Kingdom, and beyond'.\", \"In May 2019, a report by Professor Guy Standing, commissioned by the Progressive Economic Forum and forwarded to John McDonnell (the acting Shadow Chancellor of the Exchequer) suggested different models for piloting basic income. Standing's report ('Basic Income as Common Dividends: Piloting a Transformative Policy. A Report for the Shadow Chancellor of the Exchequer)' cites a 'perfect storm' of factors that lead to the need for a basic income: Broad ethical justifications for basic income, which Standing cites as 'social justice, security, freedom and solidarity' are now working in combination with urgent socio-economic demands.\", 'A basic income is defined in the report as a policy that guarantees all members of a society a minimum amount of income. One type of basic income considered is the most well-known: a universal basic income (UBI). With a UBI, everyone receives a cash transfer at regular intervals, with no conditions, and no eligibility requirements except residency in the jurisdiction.', 'Congress eventually approved a guaranteed minimum income for the elderly and the disabled. In the mid-1970s the main competitor to basic income and negative income tax, the Earned income tax credit (EITC), or its advocates, won over enough legislators for the US Congress to pass laws on that policy. In 1986, the Basic Income European Network, later renamed to Basic Income Earth Network (BIEN), was founded, with academic conferences every second year.', 'The Queensland Greens were the first Australian party to adopt a Guaranteed Adequate Income (GAI) policy in 1999. Basic Income Guarantee Australia was accepted into the Basic Income Earth Network in 2006 as an affiliate member. In August 2014, ACOSS made a recommendation to simplify the welfare system via a basic income support payment; however, this differs from a universal guaranteed income in that it would still be means-tested.'], 'kp': [('organisation basic income uk', 0.8251), ('basic income standing cites', 0.7328), ('basic income ubi', 0.8611)]}, {'evidence': ['Paul Krugman. Paul Krugman obtained his B.A. from Yale in 1974 and a PhD in economics from MIT in 1977.', 'Strategic Trade Policy and the New International Economics, ed. Paul Krugman. MIT Press, Cambridge Mass.', \"(with Paul Krugman and Daniel Tsiddon), American Economic Review, December 1993, pp. 1211 1219. 'Technology and Life Cycle of Cities'(with Paul Krugman), Journal of Economic Growth, December 1997, pp.\", \"But it's their 'right' to behave like a bunch of utter morons.' On the other hand, many economists and business analysts endorsed the coin as a way to counter threats by congressional Republicans to force the country into default by refusing to raise the debt limit. Paul Krugman said, 'So minting the coin would be undignified, but so what?\", \"Later in 2011, Krugman publicly endorsed market monetarist policy recommendations, suggesting 'a Fed regime shift' to 'expectations-based monetary policy,' and commending market monetarism for its focus on nominal GDP. Krugman used the term 'market monetarism' in his widely read blog. Also, in the fourth quarter of 2011, The Milken Institute released a study by Clark Johnson, advocating market monetarist approaches.\"], 'kp': [('paul krugman obtained yale', 0.7841), ('strategic trade policy new', 0.8215), ('cities paul krugman journal', 0.7116)]}, {'evidence': ['Income taxes and cash benefits traditionally play an important role in redistributing income in Sweden, reducing inequality among the working-age population by about 28% (the OECD average [Of what?] is 25%). This redistributive effect has diminished over time, however, as it [What?]', \"Debate continues over whether a public policy response is appropriate to income inequality. For example, Federal Reserve Economist Thomas Garrett wrote in 2010: 'It is important to understand that income inequality is a byproduct of a well-functioning capitalist economy. Individuals' earnings are directly related to their productivity ... A wary eye should be cast on policies that aim to shrink the income distribution by redistributing income from the more productive to the less productive simply for the sake of 'fairness.''\", 'Taxes. Income taxes provide one mechanism for addressing after-tax inequality. Increasing the effective progressivity of income taxes reduces the gap between higher and lower incomes.', 'This suggests that non-EU workers may have a tougher time finding high-skilled jobs than getting low-skilled work. Although income inequality between low-skilled and high-skilled workers is increasing, low-skilled workers are generally very well paid, inequality remains low and the lower and higher classes alike enjoy a very generous universal welfare state. Income taxes and cash benefits traditionally play an important role in redistributing income in Sweden, reducing inequality among the working-age population by about 28% (the OECD average [Of what?]', 'NYU economist William Baumol found that substantial inequality does not stimulate growth because poverty reduces labor force productivity. Economists Dierk Herzer and Sebastian Vollmer found that increased income inequality reduces economic growth, but growth itself increases income inequality. A 1996 study by Perotti examined the channels through which inequality may affect economic growth.'], 'kp': [('redistributing income sweden reducing', 0.8186), ('income inequality byproduct functioning', 0.6669), ('mechanism addressing tax inequality', 0.7644)]}]}]\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_ev[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/322 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "249d353d73c8454f96b5ca5fa8c8d6ed"
      },
      "application/json": {
       "n": 0,
       "total": 322,
       "elapsed": 0.008003950119018555,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(args, convert_to_tensor=True)\n",
    "\n",
    "def cosine_similarity(fuck_you):\n",
    "    #print(embeddings[0], embeddings[1])\n",
    "    #cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    cosine_scores = util.cos_sim(embeddings[0], embeddings[1:])\n",
    "\n",
    "    return cosine_scores\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section I Why is Basic Income Increasingly Popular?\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "Basic income is a policy that has broad support from both the progressive left and libertarian right.\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "Centerleft economists including Paul Krugman have endorsed the scheme for various reasons.\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "BI also reduces inequality by redistributing income from capital to labor.\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 0.6785, 0.7864, 0.7096]])\n"
     ]
    }
   ],
   "source": [
    "ev = retrieved_ev[0:1]\n",
    "def score_passages(ev):\n",
    "    ev = ev[0]\n",
    "    adus = [i for i in ev[0][\"argument\"]]\n",
    "    ret = [i for i in ev[0][\"retrieved\"]]\n",
    "\n",
    "    for i, j in zip(adus, ret):\n",
    "        ranked = []\n",
    "        print(i[\"sentence\"])\n",
    "        for _ in j[\"evidence\"]:\n",
    "            sentences = [_]\n",
    "            sentences.append(i)\n",
    "\n",
    "\n",
    "            score = cosine_similarity(sentences)\n",
    "            print(score)\n",
    "            # ranked = []\n",
    "            # ranked.append((ret, score_passages(i, j)))\n",
    "\n",
    "score_passages(ev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### SCORE TF-KEYWORD OVERLAP ###\n",
    "def overlap_score(evidence_kp, adu_kp):\n",
    "    score = 0\n",
    "    # Split Keyphrase into components, scoring partial units as overlap\n",
    "    for i in evidence_kp:\n",
    "        for j in i.split():\n",
    "            # Ensure string value, to enact .find\n",
    "            if \", \".join([i for i in adu_kp]).find(j) != -1: score += 1\n",
    "\n",
    "            else: continue\n",
    "    return score\n",
    "\n",
    "### RANK PASSAGES ###\n",
    "def score_passages(ev):\n",
    "    for _ in range(0, len(ev[0][\"argument\"])):\n",
    "        print(_)\n",
    "    # for adu in arg and passage\n",
    "        # rank passages\n",
    "\n",
    "\n",
    "    # for ln in ev:\n",
    "    #     for _ in range(0, len(ln[\"argument\"])):\n",
    "    #         adu = ln[\"argument\"][_][\"sentence\"]\n",
    "    #         adu_kp = ln[\"argument\"][_][\"kp\"]\n",
    "    #         adu_stance = ln[\"argument\"][_][\"stance\"]\n",
    "    #         target = adu_kp[0]\n",
    "    #\n",
    "    #         retrieved = ln[\"retrieved\"][_][\"evidence\"]\n",
    "    #         retrieved_kp = ln[\"retrieved\"][_][\"kp\"]\n",
    "    #         overlap = overlap_score(retrieved_kp, adu_kp)\n",
    "    #\n",
    "    #         # yield ranked, top-3\n",
    "    #         #sorted(i, key=lambda y: y[\"overlap\"], reverse=True)[0:k]\n",
    "    #\n",
    "    #         compared_stance = compare_stance(retrieved, target)\n",
    "    #         if compared_stance != adu_stance:\n",
    "    #\n",
    "    #             yield {\n",
    "    #                 \"adu\": adu,\n",
    "    #                 \"adu_kp\": adu_kp,\n",
    "    #                 \"evidence_unit\": retrieved,\n",
    "    #                 \"evidence_kps\": retrieved_kp,\n",
    "    #                 \"overlap\": overlap,\n",
    "    #                 \"evidence_stance\": compared_stance,\n",
    "    #                 \"adu_stance\": adu_stance\n",
    "    #             }\n",
    "    #\n",
    "    #             results.append({\n",
    "    #\n",
    "    #             \"argument\": [i for i in adus],\n",
    "    #             \"retrieved\": [i for i in retrieved]})\n",
    "\n",
    "# ### SCORED EVIDENCE ###\n",
    "# def score_evidence(retrieved_evidence):\n",
    "#     for ev in retrieved_ev:\n",
    "#         yield [i for i in score_passages(ev)]\n",
    "#\n",
    "# ### RANKED EVIDENCE ###\n",
    "# def rank_filter_counter_evidence(retrieved_evidence, k=3):\n",
    "#     with tqdm(total=(len(retrieved_ev))) as pbar:\n",
    "#         for i in score_evidence(retrieved_ev):\n",
    "#             yield sorted(i, key=lambda y: y[\"overlap\"], reverse=True)[0:k]\n",
    "#\n",
    "#             pbar.update()\n",
    "#\n",
    "# ### SELECT TOP-K COUNTER-EVIDENCE ###\n",
    "# tic = time.time()\n",
    "# ranked_sorted_evidence = [i for i in rank_filter_counter_evidence(retrieved_ev)]\n",
    "# ranked_sorted_evidence\n",
    "# toc = time.time()\n",
    "#\n",
    "# print(toc - tic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'adu': 'Section I Why is Basic Income Increasingly Popular?',\n  'adu_kp': ['basic income increasingly popular',\n   'basic income increasingly',\n   'section basic income increasingly'],\n  'evidence_unit': \"Brazil. Minimum income has been increasingly accepted by the Brazilian government. In 2004, President\\xa0Lula da Silva signed into law a bill to establish a universal basic income. Committee member Lady Rhys-Williams argued that the incomes for adults should be more like a basic income. She was also the first to develop the negative income tax model. Her son Brandon Rhys Williams proposed a basic income to a parliamentary committee in 1982, and soon after that in 1984, the Basic Income Research Group, now the Citizen's Basic Income Trust, began to conduct and disseminate research on basic income. Many technology experts and technology entrepreneurs have begun endorsing basic income in the 2000s and 2010s. These include Marshal Brain, Sam Altman, James Hughes, Facebook co-founder Chris Hughes, Elon Musk, and Mark Zuckerberg (in his 2017 Harvard commencement speech), and Jeremy Rifkin. The overriding theme among technologists who favor basic income is the belief that automation is creating an increasingly unstable labor market. Several British academics have been involved in the basic income debate. Among them the following:Organisations. The organisation Basic Income UK is 'a collective of independent people promoting unconditional basic income as a progressive social policy for the United Kingdom, and beyond'. Basic Income. Delaney opposes implementing a basic income (also known as a universal basic income). Minimum wage.\",\n  'evidence_kps': ['proposed basic income',\n   'basic income develop',\n   'basic income uk'],\n  'overlap': 6,\n  'evidence_stance': 'PRO',\n  'adu_stance': 'NEUTRAL'}]"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Zipping retrieved evidence, args\n",
    "print(len(ranked_sorted_evidence), len(args[0:100]))\n",
    "\n",
    "ranked_sorted_evidence[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "fout = open(\"../data/cmv_rr.jsonl\", \"w\")\n",
    "\n",
    "args = [json.loads(ln) for ln in open(\"../data/cmv_processed.jsonl\")]\n",
    "sample = args[0:100]\n",
    "\n",
    "for arg, retrieved in zip(sample, ranked_sorted_evidence):\n",
    "    adu_ = []\n",
    "    for adu in retrieved:\n",
    "        adu_.append({\n",
    "            \"evidence\": adu[\"evidence_unit\"],\n",
    "            \"evidence_kp\": adu[\"evidence_kps\"],\n",
    "        })\n",
    "\n",
    "    fout.write(json.dumps({\n",
    "        \"id\": arg[\"id\"],\n",
    "        \"argument\": arg[\"argument\"],\n",
    "        \"argument_kp\": arg[\"argument\"][\"arg_keyphrases\"],\n",
    "        \"counter\": arg[\"counter\"][\"counter\"],\n",
    "        \"counter_kp\": arg[\"counter\"][\"counter_keyphrases\"],\n",
    "        \"retrieved\": adu_\n",
    "    }))\n",
    "\n",
    "    fout.write(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "### EVALUATE OUTPUT ###\n",
    "train = [json.loads(ln) for ln in open(\"../data/cmv_rr.jsonl\", \"r\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'argument': 'Just a note I do not believe in Nazism. Now I know this view is controversial and I know its generally frowned upon but my lifelong belief is that eugenics isnt that bad. This started before I can even remember. I felt that people who live off of welfare and do nothing all day but drugs and get fat should lose their reproductive rights. At no time I believed people should die.', 'arg_keyphrases': ['isnt that bad', 'note', 'view is controversial', 'generally frowned'], 'arg_stance': ['PRO', 'Nazism']} \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'adu'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [127]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m _ \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(train))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(train[_][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margument\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[43m_\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(train[_][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrieved\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mKeyError\u001B[0m: 'adu'"
     ]
    }
   ],
   "source": [
    "_ = random.randint(0, len(train))\n",
    "print(train[_][\"argument\"], \"\\n\")\n",
    "print(train[_][\"adu\"], \"\\n\")\n",
    "print(train[_][\"retrieved\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.97145128250122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from multiprocessing.pool import ThreadPool as Pool\n",
    "# from yake import KeywordExtractor\n",
    "# import tqdm.notebook as tqdm\n",
    "# import time\n",
    "# from summa import keywords\n",
    "# from tqdm import tqdm\n",
    "#\n",
    "# ### PASSAGE RANKING; KEYWORD OVERLAP ###\n",
    "# kw_extractor = KeywordExtractor(lan=\"en\", n=3, top=5)\n",
    "#\n",
    "# # TODOs: For each ADU, Rank Merged Evidence using Keyword Overlap and Filter for Contrasting Stance\n",
    "# # TODOs: Handel Multiple Keywords\n",
    "#\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "#     # TODOs: Robust 'None' handeling\n",
    "#     if adu_kp == None:\n",
    "#         return score\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     else:\n",
    "#         for i in evidence_kp:\n",
    "#             for j in i.split():\n",
    "#                 # Ensure string value, to enact .find\n",
    "#                 if \", \".join([i for i in adu_kp]).find(j) != -1: score += 1\n",
    "#\n",
    "#                 else: continue\n",
    "#\n",
    "#     return score\n",
    "#\n",
    "# def calculate_overlap(merged_ev, adu_kp):\n",
    "#\n",
    "#     for ev_unit in sentences_segment(merged_ev):\n",
    "#         toks = tokeniser(ev_unit)\n",
    "#         kp_overlap = 0\n",
    "#\n",
    "#         if len(toks) <= 8: continue\n",
    "#\n",
    "#         #ev_unit_kp = [i for i in keywords.keywords(ev_unit).split(\"\\n\")]\n",
    "#         ev_unit_kp = [i[0] for i in kw_extractor.extract_keywords(ev_unit)]\n",
    "#\n",
    "#         if ev_unit_kp:\n",
    "#             kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "#\n",
    "#         else: ev_unit_kp = None\n",
    "#         yield ev_unit, ev_unit_kp, kp_overlap\n",
    "#\n",
    "# # pool = Pool(8)\n",
    "# ### RANK PASSAGES ###\n",
    "# def score_passages(ev_):\n",
    "#     adu = ev_[0][\"argument_discourse_unit\"]\n",
    "#     adu_stance = ev_[0][\"adu_stance\"]\n",
    "#     merged_ev = ev_[0][\"merged_evidence\"]\n",
    "#     adu_kp = ev_[0][\"adu_keyphrases\"]\n",
    "#\n",
    "#     ### CALCULATE OVERLAP ###\n",
    "#     for ev_unit, ev_unit_kp, kp_overlap in calculate_overlap(merged_ev, adu_kp):\n",
    "#         target = adu_kp[0]\n",
    "#\n",
    "#         compared_stace = compare_stance(ev_unit, target)\n",
    "#         if compared_stace != adu_stance:\n",
    "#             yield {\n",
    "#                 \"adu\": adu,\n",
    "#                 \"adu_kp\": adu_kp,\n",
    "#                 \"evidence_unit\": ev_unit,\n",
    "#                 \"evidence_kps\": ev_unit_kp,\n",
    "#                 \"overlap\": kp_overlap,\n",
    "#                 \"evidence_stance\": compare_stance(ev_unit, target),\n",
    "#                 \"adu_stance\": adu_stance\n",
    "#             }\n",
    "#\n",
    "#         else: continue\n",
    "#\n",
    "# ### SCORED EVIDENCE ###\n",
    "# def score_evidence(retrieved_evidence):\n",
    "#     for ev_ in retrieved_ev:\n",
    "#         yield [i for i in score_passages(ev_)]\n",
    "#\n",
    "# ### RANKED EVIDENCE ###\n",
    "# def rank_filter_counter_evidence(retireved_evidence, k=3):\n",
    "#     with tqdm(total=(len(retrieved_ev))) as pbar:\n",
    "#         for i in score_evidence(retrieved_ev):\n",
    "#             yield sorted(i, key=lambda y: y[\"overlap\"], reverse=True)[0:k]\n",
    "#\n",
    "#             pbar.update()\n",
    "#\n",
    "#\n",
    "# ### SELECT TOP-K COUNTER-EVIDENCE ###\n",
    "# tic = time.time()\n",
    "# ranked_sorted_evidence = [i for i in rank_filter_counter_evidence(retrieved_ev)]\n",
    "# ranked_sorted_evidence\n",
    "# toc = time.time()\n",
    "#\n",
    "# print(toc - tic)\n",
    "# # TIME 1:20M"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section I Why is Basic Income Increasingly Popular?\n",
      "['basic income increasingly popular', 'basic income increasingly', 'section basic income increasingly']\n",
      "\n",
      "Brazil. Minimum income has been increasingly accepted by the Brazilian government. In 2004, President Lula da Silva signed into law a bill to establish a universal basic income. Committee member Lady Rhys-Williams argued that the incomes for adults should be more like a basic income. She was also the first to develop the negative income tax model. Her son Brandon Rhys Williams proposed a basic income to a parliamentary committee in 1982, and soon after that in 1984, the Basic Income Research Group, now the Citizen's Basic Income Trust, began to conduct and disseminate research on basic income. Many technology experts and technology entrepreneurs have begun endorsing basic income in the 2000s and 2010s. These include Marshal Brain, Sam Altman, James Hughes, Facebook co-founder Chris Hughes, Elon Musk, and Mark Zuckerberg (in his 2017 Harvard commencement speech), and Jeremy Rifkin. The overriding theme among technologists who favor basic income is the belief that automation is creating an increasingly unstable labor market. Several British academics have been involved in the basic income debate. Among them the following:Organisations. The organisation Basic Income UK is 'a collective of independent people promoting unconditional basic income as a progressive social policy for the United Kingdom, and beyond'. Basic Income. Delaney opposes implementing a basic income (also known as a universal basic income). Minimum wage.\n",
      "['proposed basic income', 'basic income develop', 'basic income uk']\n",
      "Basic income is a policy that has broad support from both the progressive left and libertarian right.\n",
      "['basic income', 'basic income policy', 'basic income policy broad']\n",
      "\n",
      "Several British academics have been involved in the basic income debate. Among them the following:Organisations. The organisation Basic Income UK is 'a collective of independent people promoting unconditional basic income as a progressive social policy for the United Kingdom, and beyond'. In May 2019, a report by Professor Guy Standing, commissioned by the Progressive Economic Forum and forwarded to John McDonnell (the acting Shadow Chancellor of the Exchequer) suggested different models for piloting basic income. Standing's report ('Basic Income as Common Dividends: Piloting a Transformative Policy. A Report for the Shadow Chancellor of the Exchequer)' cites a 'perfect storm' of factors that lead to the need for a basic income: Broad ethical justifications for basic income, which Standing cites as 'social justice, security, freedom and solidarity' are now working in combination with urgent socio-economic demands. A basic income is defined in the report as a policy that guarantees all members of a society a minimum amount of income. One type of basic income considered is the most well-known: a universal basic income (UBI). With a UBI, everyone receives a cash transfer at regular intervals, with no conditions, and no eligibility requirements except residency in the jurisdiction. Congress eventually approved a guaranteed minimum income for the elderly and the disabled. In the mid-1970s the main competitor to basic income and negative income tax, the Earned income tax credit (EITC), or its advocates, won over enough legislators for the US Congress to pass laws on that policy. In 1986, the Basic Income European Network, later renamed to Basic Income Earth Network (BIEN), was founded, with academic conferences every second year. The Queensland Greens were the first Australian party to adopt a Guaranteed Adequate Income (GAI) policy in 1999. Basic Income Guarantee Australia was accepted into the Basic Income Earth Network in 2006 as an affiliate member. In August 2014, ACOSS made a recommendation to simplify the welfare system via a basic income support payment; however, this differs from a universal guaranteed income in that it would still be means-tested.\n",
      "['basic income uk', 'organisation basic income uk', 'basic income uk collective']\n",
      "Centerleft economists including Paul Krugman have endorsed the scheme for various reasons.\n",
      "['paul krugman endorsed scheme', 'economists including paul krugman', 'krugman endorsed scheme various']\n",
      "\n",
      "Paul Krugman. Paul Krugman obtained his B.A. from Yale in 1974 and a PhD in economics from MIT in 1977. Strategic Trade Policy and the New International Economics, ed. Paul Krugman. MIT Press, Cambridge Mass. (with Paul Krugman and Daniel Tsiddon), American Economic Review, December 1993, pp. 1211 1219. 'Technology and Life Cycle of Cities'(with Paul Krugman), Journal of Economic Growth, December 1997, pp. But it's their 'right' to behave like a bunch of utter morons.' On the other hand, many economists and business analysts endorsed the coin as a way to counter threats by congressional Republicans to force the country into default by refusing to raise the debt limit. Paul Krugman said, 'So minting the coin would be undignified, but so what? Later in 2011, Krugman publicly endorsed market monetarist policy recommendations, suggesting 'a Fed regime shift' to 'expectations-based monetary policy,' and commending market monetarism for its focus on nominal GDP. Krugman used the term 'market monetarism' in his widely read blog. Also, in the fourth quarter of 2011, The Milken Institute released a study by Clark Johnson, advocating market monetarist approaches.\n",
      "['krugman said minting coin', 'economics ed paul krugman', 'paul krugman journal economic']\n",
      "BI also reduces inequality by redistributing income from capital to labor.\n",
      "['bi reduces inequality redistributing', 'reduces inequality redistributing income', 'inequality redistributing income capital']\n",
      "\n",
      "Income taxes and cash benefits traditionally play an important role in redistributing income in Sweden, reducing inequality among the working-age population by about 28% (the OECD average [Of what?] is 25%). This redistributive effect has diminished over time, however, as it [What?] Debate continues over whether a public policy response is appropriate to income inequality. For example, Federal Reserve Economist Thomas Garrett wrote in 2010: 'It is important to understand that income inequality is a byproduct of a well-functioning capitalist economy. Individuals' earnings are directly related to their productivity ... A wary eye should be cast on policies that aim to shrink the income distribution by redistributing income from the more productive to the less productive simply for the sake of 'fairness.'' Taxes. Income taxes provide one mechanism for addressing after-tax inequality. Increasing the effective progressivity of income taxes reduces the gap between higher and lower incomes. This suggests that non-EU workers may have a tougher time finding high-skilled jobs than getting low-skilled work. Although income inequality between low-skilled and high-skilled workers is increasing, low-skilled workers are generally very well paid, inequality remains low and the lower and higher classes alike enjoy a very generous universal welfare state. Income taxes and cash benefits traditionally play an important role in redistributing income in Sweden, reducing inequality among the working-age population by about 28% (the OECD average [Of what?] NYU economist William Baumol found that substantial inequality does not stimulate growth because poverty reduces labor force productivity. Economists Dierk Herzer and Sebastian Vollmer found that increased income inequality reduces economic growth, but growth itself increases income inequality. A 1996 study by Perotti examined the channels through which inequality may affect economic growth.\n",
      "['income sweden reducing inequality', 'redistributing income sweden reducing', 'redistributing income sweden']\n"
     ]
    }
   ],
   "source": [
    "# idx = 2\n",
    "# for ln in retrieved_ev:\n",
    "#     r = ln[0]\n",
    "#     for _ in range(0, len(r[\"argument\"])):\n",
    "#         print(r[\"argument\"][_][\"sentence\"])\n",
    "#         print(r[\"argument\"][_][\"kp\"])\n",
    "#         print(\"\")\n",
    "#         print(r[\"retrieved\"][_][\"evidence\"])\n",
    "#         print(r[\"retrieved\"][_][\"kp\"])\n",
    "\n",
    "#\"counter\": {\"counter\": arg[\"counter\"][\"counter\"], \"counter_kp\": arg[\"counter\"][\"counter_keyphrases\"]}\n",
    "# \"argument_discourse_unit\": adu,\n",
    "# \"query\": query,\n",
    "# \"adu_keyphrases\": [i for i in kp],\n",
    "# \"adu_stance\": sentence_stance(adu, kp),\n",
    "# \"merged_evidence\": \", \".join(ln for ln in evidence)\n",
    "# \"retrieved_documents_titles\": titles,\n",
    "# \"retrieved_evidence\": evidence,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # TODOs: Speed-up, Parrelleise, Yield\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "\n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: score += 1\n",
    "\n",
    "#             else: continue\n",
    "\n",
    "#     return score\n",
    "\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity']\n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "\n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "\n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu,\n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp,\n",
    "#         \"kp_overlap\": kp_overlap\n",
    "\n",
    "#         })\n",
    "\n",
    "# adu_ev_overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### OVERLAP RANKED EVIDENCE ###\n",
    "\n",
    "# adu_ev_overlap.sort(key=lambda y: y[\"kp_overlap\"], reverse=True)\n",
    "# adu_ev_overlap\n",
    "\n",
    "# ### FILTER IRRELEVANT EVIDENCE ###\n",
    "# overlapping = [i for i in adu_ev_overlap if i[\"kp_overlap\"] !=0]\n",
    "\n",
    "# len(adu_ev_overlap), len(overlapping)\n",
    "# overlapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'PRO'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stance Test\n",
    "# adu = 'I cant remember the topic that spurred this discussion but a friend and I were debating whether manmade things were natural.'\n",
    "# ev_unit = 'In this essay, Mill argues the idea that the morality of an action can be judged by whether it is natural or unnatural.'\n",
    "# target = 'natural things'\n",
    "#\n",
    "# stance = compare_stance(ev_unit, target)\n",
    "# stance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### ASSERT SAME STANCE ###\n",
    "# from detection.stance_classifier import sentence_stance, compare_stance\n",
    "#\n",
    "# # TODOs: Ensure KPs Extracts are constrained to 1 unit\n",
    "# opposing_stance = []\n",
    "# for i in overlapping:\n",
    "#     adu = i[\"adu\"]\n",
    "#     target = \" \".join(i for i in i[\"adu_kp\"])\n",
    "#     ev_unit = i[\"ev_unit\"]\n",
    "#\n",
    "#     ev_stance = compare_stance(ev_unit, ev_unit, target)\n",
    "#     adu_stance = sentence_stance(adu, target)\n",
    "#\n",
    "#     if ev_stance != adu_stance:\n",
    "#         opposing_stance.append((ev_unit, ev_stance, adu_stance))\n",
    "#\n",
    "#     else: continue\n",
    "#\n",
    "# opposing_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### RANKING ###\n",
    "\n",
    "# TODOs: Speed-up, Parrelleise, Yield\n",
    "# ev_units = evidence\n",
    "# adu_kp = extract_keyphrase(adu)\n",
    "\n",
    "# adu_ev_overlap = []\n",
    "\n",
    "# kp_1 = ['sex', 'relationship', 'opportunity'] \n",
    "# kp_2 = ['better sex']\n",
    "\n",
    "# overlap_score(kp_2, kp_1)\n",
    "\n",
    "# for ev_unit in evidence:\n",
    "#     #print(ev_unit)\n",
    "#     toks = tokeniser(ev_unit)\n",
    "\n",
    "#     # Exprimental Value\n",
    "#     if len(toks) <= 8:\n",
    "#         continue\n",
    "    \n",
    "#     ev_unit_kp = extract_keyphrase(ev_unit)\n",
    "#     kp_overlap = overlap_score(evidence_kp=ev_unit_kp, adu_kp=adu_kp)\n",
    "    \n",
    "#     adu_ev_overlap.append({\n",
    "#         \"adu\": adu, \n",
    "#         \"adu_kp\": adu_kp,\n",
    "#         \"ev_unit\": ev_unit,\n",
    "#         \"ev_unit_kp\": ev_unit_kp, \n",
    "#         \"kp_overlap\": kp_overlap\n",
    "        \n",
    "#         })\n",
    "        \n",
    "# adu_ev_overlap\n",
    "\n",
    "\n",
    "#rank_passages(retrieved_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# # TODOs: Package as a Module\n",
    "# # TODOs: Handle Negation (Polarity shifters)\n",
    "# # TODOs: Review Unsuperived Approach; Consider adveanced patterns and common-sence knowledge\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# sentence = \"I hate abortion rights. Abortions should be banned.\"\n",
    "# sentence_2 = \"I like abortion rights. I belive we should keep them.\"\n",
    "# sentence_3 = \"I hate tennis. People should play tennis more often\"\n",
    "\n",
    "# ### STANCE SCORING ###\n",
    "\n",
    "# # TODOs: https://www.cs.uic.edu/~liub/FBS/opinion-mining-final-WSDM.pdf \n",
    "# # TODOs: Pattern based Negation\n",
    "# # TODOs: Semantic Orientation of an opinion (Claim)\n",
    "# # TODOs:Group synonyms of 'features', 'targets'\n",
    "\n",
    "# phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# ### SENTIMENT LEXICONS ###\n",
    "# pos = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/positive_lex.txt\")]\n",
    "# neg = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/negative_lex.txt\")]\n",
    "# polarity_shifters = [w.replace(\"\\n\", \"\") for w in open(\"../../data/lexicon/shifter_lexicon.txt\")]\n",
    "\n",
    "# ### STANCE: ASPECT-SEMANTIC ORIENTATION ###\n",
    "# def extract_aspect(sentence, n_gram):\n",
    "#     aspects = extract_keyphrase(str(sentence))[0]\n",
    "\n",
    "#     return nlp(aspects)\n",
    "\n",
    "# def index_aspect(phrase, aspect, sentence):    \n",
    "#     patterns = [nlp(aspect)]\n",
    "#     phrase_matcher.add(phrase, None, *patterns)\n",
    "\n",
    "#     start = 0\n",
    "#     stop = 0\n",
    "\n",
    "#     matched_phrases = phrase_matcher(sentence)\n",
    "#     for i in matched_phrases:\n",
    "#         _, start, stop = i\n",
    "        \n",
    "#     return start, stop\n",
    "\n",
    "# # TODOs: Implement Polarity Shifters, Simple\n",
    "# # TODOs: Implement Polarity Shifters, Complex, Verb Patterns\n",
    "# def stance_score(start, stop, sentence):\n",
    "#     pos_score = 0.0\n",
    "#     neg_score = 0.0\n",
    "\n",
    "#     score = 0\n",
    "#     for idx, tok in enumerate(sentence):\n",
    "#         if idx == start or idx == stop:\n",
    "#             continue\n",
    "\n",
    "#         # TODOs: Implement Polarity Shift\n",
    "#         # TODOs: Experiement with descriptive term + keyphrase aspects\n",
    "#         # TODOs: ABSA https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis\n",
    "#         # Use external libaray: Textblob\n",
    "        \n",
    "#         k = 8\n",
    "#         # Negation Rules\n",
    "#         shifted_tok = None\n",
    "#         shifted_toks = []\n",
    "\n",
    "#         if (tok.dep_ == \"neg\") or (tok.dep_ in polarity_shifters):\n",
    "#             #Shift to Negative\n",
    "#             if idx <= k:\n",
    "#                 if idx < start: neg_score += 1/(start - idx)\n",
    "#                 else: neg_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#             if shifted_tok != None and shifted_tok in neg:\n",
    "#                 print(shifted_tok.text)\n",
    "#                 # Shift to Positive\n",
    "#                 if idx < start: pos_score += 1/(start - idx)\n",
    "#                 elif idx > start: pos_score += 1/(idx - stop)**0.5\n",
    "#                 else: continue\n",
    "\n",
    "#         # Aspect Sentement Orientation\n",
    "#         if tok.text in pos:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "            \n",
    "#             if idx < start: pos_score += 1/(start - idx)\n",
    "#             else: pos_score += 1/(idx - stop)**0.5\n",
    "\n",
    "#         if tok.text in neg:\n",
    "#             if tok in shifted_toks:\n",
    "#                 continue\n",
    "\n",
    "#             if idx <= start: neg_score += 1/(start - idx)\n",
    "#             else: neg_score += 1/(idx - stop)**0.5\n",
    "    \n",
    "#     score = pos_score - neg_score /(pos_score + neg_score + 1)\n",
    "\n",
    "#     return score\n",
    "\n",
    "# def overlap_score(evidence_kp, adu_kp):\n",
    "#     score = 0\n",
    "    \n",
    "#     # Split Keyphrase into components, scoring partial units as overlap\n",
    "#     for i in evidence_kp:\n",
    "#         for j in i.split():\n",
    "#             # Ensure string value, to enact .find\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 score += 1\n",
    "#                 token = j\n",
    "            \n",
    "#             else: continue\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def get_overlapping_token(evidence_kp, adu_kp):\n",
    "#     for i in evidence_kp:\n",
    "#         overlap_tokens = []\n",
    "#         for j in i.split():\n",
    "#             if \" \".join(adu_kp).find(j) != -1: \n",
    "#                 overlap_tokens.append(j) \n",
    "            \n",
    "#         return \" \".join(i for i in overlap_tokens)\n",
    "\n",
    "# def sentence_stance(sentence, aspect):\n",
    "#     sentence = nlp(sentence)\n",
    "\n",
    "#     start, stop = index_aspect(\"aspects\", aspect, sentence)\n",
    "#     score = stance_score(start, stop, sentence)\n",
    "\n",
    "#     # Add Neutral\n",
    "#     #stance = {\"claim\": sentence, \"stance\": \"PRO\", \"aspect\": aspect} if score > 0 else {\"claim\": sentence, \"stance\": \"CON\", \"aspect\": aspect}\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# def fuzzy_match(target, evidence_unit):\n",
    "\n",
    "#     overlapping_aspect = process.extractOne(target, ev.split())[0]\n",
    "#     score = overlapping_aspect[1]\n",
    "\n",
    "#     overlapping_aspect = nlp(re.sub(r'[^\\w]', ' ', overlapping_aspect))\n",
    "\n",
    "#     return overlapping_aspect, score\n",
    "\n",
    "# def compare_stance(ev_unit, evidence_aspect, adu_target):\n",
    "#     # Note: Already identified mathcing or partially matching Aspects. \n",
    "\n",
    "#     # Get the overlapping evidence aspect-target.\n",
    "#     overlapping_target, score = fuzzy_match(target=adu_aspect, evidence_unit=ev)\n",
    "    \n",
    "#     # Get position of the overlapping_target\n",
    "#     start, stop = index_aspect(\"OVERLAP\", nlp(overlapping_target), nlp(ev_unit))\n",
    "\n",
    "#     # Assert Stance towards evidence aspect\n",
    "#     score = stance_score(start, stop, nlp(ev_unit))\n",
    "    \n",
    "#     return \"PRO\" if score > 0 else \"CON\"\n",
    "\n",
    "# ev = \"These simple ideas and techniques could help both you and your lover enjoy sex. 1 / 10 Getty Images/Caiaimage Think beyond the thrust.\"\n",
    "# ev_aspect = \"sex\", \"relationship\", \"opportunity\"\n",
    "\n",
    "# adu = 'Hello! Let me preface by saying I dont believe there is a better sex.'\n",
    "# adu_aspect = \"better sex\"\n",
    "\n",
    "# print(sentence_stance(\"The mutual trust and understanding you share with your partner will lead to better sex, but that's not the only reason sex can be better when you're not in a relationship.\", adu_aspect))\n",
    "# print(compare_stance(ev, ev_aspect, adu_aspect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from spacy.matcher import DependencyMatcher, Matcher\n",
    "# matcher = Matcher(vocab=nlp.vocab)\n",
    "# matcher\n",
    "\n",
    "# # Matching Rule: Pronouns with Verbs that follow them\n",
    "# aspect = \"better sex\"\n",
    "# patterns = [\n",
    "#     [{\"DEP\": \"neg\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"DEP\": \"neg\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"LOWER\": aspect}],\n",
    "#     [{\"LOWER\": aspect.lower()}]\n",
    "# ]\n",
    "\n",
    "# test = nlp(\"Hello! Let me preface by saying I dont believe there is a not better sex.\")\n",
    "# test_2 = nlp(\"These simple ideas and techniques could help both you and your lover enjoy better sex.\")\n",
    "\n",
    "# matcher.add(\"test\", patterns=patterns)\n",
    "# result = matcher(test_2, as_spans=True)\n",
    "\n",
    "# result\n",
    "\n",
    "# # for tok in test:\n",
    "# #     print(tok.i, tok, tok.pos_, tok.dep_, tok.head.i, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### TARGETED RETRIEVAL: ATTACKING PEMISES ###\n",
    "\n",
    "# from BERT_adu_classifier import predict\n",
    "\n",
    "# premises = []\n",
    "# for sent in sentences:\n",
    "#     prediction = predict(sent)\n",
    "    \n",
    "#     if prediction == \"premise\":\n",
    "#         premises.append(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b747fec5972a5a28202124dfae2950631b4721a6e18efe99aaae23c73408484"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}